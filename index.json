[{"authors":null,"categories":null,"content":"I’m an autonomy engineer who’s passionate about quadcopters, flight vehicles and just about anything that moves. This space is a showcase of some of the cool projects I’ve worked on recently. More about me here.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"fbff766363f2e44b2ad29c64d9e35a6a","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I’m an autonomy engineer who’s passionate about quadcopters, flight vehicles and just about anything that moves. This space is a showcase of some of the cool projects I’ve worked on recently.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"I’m an engineer who’s been passionate about vehicles, robotics and dynamic systems as long as I can remember. Quadrotors, planes, racecars and spacecraft are all systems that I’ve found exciting to work on or simply watch. While my past focus was on the mechanical and dynamics aspects, my interests have shifted to the software side and the sensing, estimation and control algorithms that make their motion possible.\nI recently graduated from the University of Toronto Institute for Aerospace Studies(UTIAS) completing coursework focused on control systems, robotics and machine learning. I’m currently seeking my next challenge and am looking to join a similarly motivated group of people developing new vehicle or robotic systems.\nMy prior background was 5 years of structures analysis and design experience on various automotive and aerospace programs. Through this I’ve developed strong capabilities in simulation, data analysis and identifying core system dynamics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"45a9e22709913af6b2ec505cbd0bc019","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I’m an engineer who’s been passionate about vehicles, robotics and dynamic systems as long as I can remember. Quadrotors, planes, racecars and spacecraft are all systems that I’ve found exciting to work on or simply watch.","tags":null,"title":"Matt Brymer","type":"authors"},{"authors":null,"categories":null,"content":"GitHub Repo | Project Report\nThis was my final project at UTIAS where I developed a visual-inertial estimation algorithm for estimating the relative pose of a quadrotor over a landing pad and deployed it on a custom hardware platform.\nOverview This work was performed as part of an overarching personal project to build a quadrotor from hobby grade components capable of autonomously taking off, flying to a target GPS location and then executing a precision landing on a landing pad at that location. One of the most crucial aspects of this system is deriving an accurate estimate of the vehicle relative pose for control of the final landing phase. Low cost GPS units typically only achieve horizontal position accuracies on the order of 2.5 m [1], which is good for general purpose or return to launch flight but insufficient for a precise landing.\nMore accurate measurements of the relative pose can be achieved via vision with an onboard camera, which has been well explored in the literature [2][3]. These pose measurements can then be fused with IMU data to achieve a high rate state estimate. Thus for this project I investigated developing a visual-inertial estimation algorithm for implementation on a low cost quadrotor.\nApproach Filter Architecture and Software Implementation To simplify the vision problem I placed an AprilTag fiducial marker on the landing pad, which are commonly used in robotics and allow the full 6 DoF pose to be estimated with a monocular camera [4]. The picture below shows the overall estimation problem and coordinate frames involved.\nI used a Multiplicative Extended Kalman Filter [5] to estimate both the relative pose and the IMU biases. This framework breaks the state up into a nominal and perturbation component that allows the attitude quaternion to be estimated with a conventional Kalman Filter, which is normally limited to vector spaces. The full details of the derivation are available in the report.\nI implemented the filter in a C++ ROS node, which takes IMU data and the pose of the tag from the AprilTag ROS node [6] as inputs and outputs the estimated relative pose and covariance. Due to the cost of the image processing pipeline, the detections reach the filter with a significant delay, which was on the order of 0.2 s in the hardware setup. For this reason the tag detections are fused to a past state based on the estimated delay.\nHardware Platform The filter was deployed on a custom quadrotor platform illustrated below. Flight control is performed by a Holybro Kakute F7 flight controller running ArduPilot 4.2.2 and the filter runs on a NVIDIA Jetson Nano 4GB using images from an industrial machine vision camera. The vehicle has a total mass of 1.115 kg and estimated maximum thrust of 36.9 N based on data from the motor manufacturer, leading to a thrust to weight ratio of 3.4.\nTo make the image processing time tractable, the camera image is downsampled to 640x480 to allow the AprilTag node to produce detections at 15 Hz. A custom landing target was designed using a bundle of 13 AprilTags of different sizes within a 0.84 m wide square, which allows for some tags to remain visible when the vehicle is offset laterally.\nValidation Simulation The filter was first validated in simulation using the TRAILab fork of the RotorS simulator [7] before testing in hardware. The flight was simulated using prerecorded open loop transmitter inputs for manual sweeping flights over the AprilTag target at heights of 2 m and 4 m. The video below for the 2 m case shows the estimated and ground truth pose visualized with RViz as well as the simulated camera view with the AprilTag detection overlaid.\nThe filter generally tracks the ground truth position and orientation well, with estimation errors below approximately 0.050 m as long as detection is maintained. The cameras loses sight of the tag multiple times during the run at the edges of its sweeping trajectory, leading to drift in the state estimate. In this example we at one point observe a drift of 0.58 m in the position estimate after 5 s of lost detection around 50 s into the event. While not ideal, this is a relatively long interval to go without correction from measurements.\nAdditional details of the orientation estimate and filter consistency are available in the report.\nHardware Testing The filter was then validated via outdoor flight tests on the quadrotor platform under manual flight. Two cases were evaluated, including a sweeping flight back and forth over the target as well as a full landing. The height in the sweeping flight case was approximately 2.5 m while the landing case began from a height of approximately 5 m.\nThe videos below show ground and onboard camera views along with the RViz visualization of the state estimate and time traces for both cases.\n$$ $$\nNo ground truth is available so the estimate is instead compared against the AprilTag reported pose for reference. While detection is maintained the filter does a good job of tracking the …","date":1661731200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661731200,"objectID":"321852a82a4d4e09ccee8faeb3784f36","permalink":"https://mbrymer.github.io/project/quadrotor_relative_pose_estimation/","publishdate":"2022-08-29T00:00:00Z","relpermalink":"/project/quadrotor_relative_pose_estimation/","section":"project","summary":"Final project where I began developing a quadrotor capable of autonomously flying to a target location and landing based on visual-inertial navigation.","tags":["Sensor Fusion","Quadrotor","Computer Vision"],"title":"Visual-Inertial Relative Pose Estimation for Quadrotor Landing","type":"project"},{"authors":null,"categories":null,"content":"GitHub Repo | Project Report | Project Partners: Vandan Rao, Furqan Ahmed, Sangita Sahu\nThis was a course project where my teammates and I studied the problem of trajectory planning for high speed quadrotor flight in unknown environments.\nOverview Motion planning in environments that are unknown apriori is challenging for traditional sampling based planners because many samples end up becoming wasted when new obstacles invalidate entire sections of the tree. Numerous alternate approaches exist for this problem class, one of which is hierarchical planners. These use a fast global planner to generate a collision free path that is then refined by a local planner to generate a dynamically feasible trajectory, usually by solving an optimization problem.\nFor our project we investigated this class of methods by implementing a simplified version of the FASTER planner [1].\nApproach For global planning, FASTER uses Jump Point Search (JPS) to generate a shortest piecewise linear path to the goal. It then performs a convex decomposition of the free space to generate a set of polytopes in which the plan can lie. The local plan is parameterized with a set of Bezier curves and an optimization problem is solved to find spline control points that minimize squared jerk and remain within vehicle dynamic limits. The convex hull property of Bezier curves reduces collision checking to keeping the control points within at least one polytope, but requires adding binary variables to allocate each segment to a polytope, promoting the problem to a MIQP.\nFASTER generates two plans, an optimistic one in both the free known and unknown space and a safe one in only the free known space with a stopping condition. This allows for higher speeds without risking collision if there is an object in the unknown space. To reduce scope for this project we planned only in the free known space, which sacrifices speed to maintain safety. The picture below shows our version, which we jokingly referred to as SLOWER.\nWe implemented the planner in a series of Python ROS nodes, one for each major step including the mapper (Furqan), global planner (Sangita), convex decomposition (Vandan) and local planner (myself). We tested it in Gazebo simulations making use of the quadrotor simulator used by the original authors in their GitHub repository [2].\nMy Contributions For my part, I lead the definition of the ROS architecture and software interfaces as well as writing the local planner and master node.\nThe local planner at each replanning iteration defines the MIQP optimization problem, calls the solver(MOSEK) and then stores the result so that the time interpolated trajectory can be output at the high frequency needed for motion. This requires managing the full set of constraints including initial/boundary conditions, continuity between segments, vehicle dynamic limits(speed, acceleration and jerk), and planar collision constraints for each polytope as well as mixed continuous and binary variables.\nThe video below shows the local planner operating in isolation using fixed global plan and convex decomposition inputs corresponding to a simple S shaped corridor. The RViz view in the lower left shows the full spline trajectory generated by the local planner. Since the inputs are fixed the cylinder forest world is for visualizing the motion only.\nFinal Results Due to implementation challenges with the global planner, it was necessary to integrate our convex decomposition and local planner with the JPS global planner implemented in the original FASTER implementation [2]. We tested this combination in both a small forest and office environment, which are illustrated below.\nSample videos of the flights in the both environments are shown below. In testing we achieved maximum speeds of up to 4.2 m/s and replanning rates of up to 10 Hz. The full results summary is available in the report here.\n$$ $$ Credits for the RViz tools to visualize the polyhedra and ellipsoids go to Sikang Liu [4].\nChallenges We encountered a number of challenges over the course of the project. The first was run time for the convex decomposition phase due to the cost of processing the incoming point cloud. Performance improved over the course of the project, but it can be seen in the videos that the decomposition still lags behind the speed of the vehicle. This delay causes the local planner to stop periodically while waiting for it to update.\nThe second was the time allocation heuristic. FASTER determines the time allocation for the trajectory by multiplying the minimum straight line time to the goal by a scale factor to account for path curvature. The scale factor is increased or decreased between replanning iterations depending on if a feasible solution is found in a form of line search to attempt to find the fastest possible trajectory time.\nThe use of infeasibility as the only stop criterion was found to not be robust and lead to scenarios where the factor was continually made more aggressive …","date":1651104e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651104e3,"objectID":"6c4d4780c50e497189cac3f0c9fe0822","permalink":"https://mbrymer.github.io/project/quadrotor_trajectory_planning/","publishdate":"2022-04-28T00:00:00Z","relpermalink":"/project/quadrotor_trajectory_planning/","section":"project","summary":"Course project for AER 1516 - Motion Planning for Robotics.","tags":["Trajectory Planning","Quadrotor"],"title":"Hierarchical Trajectory Planning for Quadrotor Flight in Unknown Environments","type":"project"},{"authors":null,"categories":null,"content":"The final assignment in one of my classes at UTIAS, AER 1513 - State Estimation for Robotics, had us solve a batch state estimation problem for a sensor head with both a stereo camera and an IMU attached.\nOverview The task was to derive an estimate for the full 6 DoF pose trajectory of the sensor head (henceforth referred to as the “vehicle”) for a sweeping motion spanning approximately 40 s given a history of IMU and stereo camera measurements. The vehicle was moved back and forth over a canvas with a set of markers on it that were each known to have a given location. These formed landmarks that could be used to establish an absolute position reference and triangulate a position and orientation.\nThe experimental setup along with sample camera images and the ground truth pose measured by an external motion capture system is shown below.\nThe camera was kept relatively close to the canvas and moved through a range of angles, leading to only some of the landmarks being visible at any given time. For about a third of the event there were fewer than 3 landmarks in view, or sometimes none at all. Thus fusing both the camera and IMU data is necessary to obtain a pose estimate for the full trajectory.\nImplementation and Results The batch estimation problem was solved by forming a nonlinear least squares cost function in terms of the pose matrix at each timestep, the IMU inputs and stereo camera measurements. The problem was solved using an iterative Gauss-Newton optimization implemented in a MATLAB script that converged in approximately 10 iterations. Some of the details on the problem formulation are given in the next section.\nA 3D trace of the final vehicle position estimate is shown below, along with the ground truth and a dead reckoned estimate from integrating the IMU signal. It can be seen that the estimate overall follows the ground truth well, although there is a consistent bias in the Y position. The dead reckoned estimate quickly diverges from the ground truth, demonstrating the importance of supplementing IMU data with absolute references.\nDetailed plots of the position and rotational estimation error when compared against ground truth are shown below. Overall the estimation errors are relatively low compared to the scale of motion in the problem, demonstrating the strength of the batch estimation approach.\nIt can be seen that the Y estimation error exhibits a clear bias towards negative error as we saw earlier in the 3D trace, with a mean error of approximately -15 mm. This is likely due to a bias in the stereo camera measurements, which was observed in provided histograms of the stereo camera measurement errors for the vertical pixel measurements.\nIt can also be seen that the $3-\\sigma$ uncertainty envelope calculated based on the estimated covariance bounds the estimation error for the most part. As expected it grows when the number of landmarks in view decreases, particularly between 20-23 s.\nOverall, the batch state estimation strategy did a great job of being able to fuse both data streams to produce a robust pose estimate that is mostly consistent with its covariance estimate. This assignment was a great opportunity to practice some challenging Jacobian derivations, optimization problem formulation, rotation formalisms and sensor fusion for states that form a group rather than a vector space, such as pose matrices.\nProblem Formulation In this section I’ll briefly explain the methodology used to give a flavour for the problem. I’ll gloss over the finer points in the interest of keeping it short. For those who are interested the full details are given in Sections 6 and 7 of Barfoot [1].\nSome simplifications were made to reduced the scope of the problem. The linear acceleration signals were transformed to linear velocities and the stereo camera images were preprocessed to identify the landmarks within both images, removing the problem of data association. Thus the inputs for the estimation problem consisted of a time history of linear velocity, angular velocity and pixel coordinates for the landmarks in both camera images.\nThe problem was formulated as a batch state estimation problem where we aim to estimate a trajectory of pose matrices $ \\mathbf{T}_{vi,k} $ for all discrete time steps within the interval, where the pose matrix encodes the world (i for inertial frame) to vehicle transform and consists of a rotation matrix and position vector as:\n$$ \\mathbf{T}_{vi,k} = \\begin{bmatrix} \\mathbf{C}_{vi,k} \u0026amp; -\\mathbf{C}_{vi,k}\\mathbf{r}_{i,k}^{vi} \\\\ \\mathbf{0}^T \u0026amp; 1 \\end{bmatrix} $$\nWhere $ \\mathbf{r}_{i,k}^{vi} $ represents the position vector pointing from the world frame to the vehicle expressed in the inertial frame and the rotation matrix $\\mathbf{C}_{vi,k}$ maps from the world to the vehicle frame at timestep $k$.\nFor simplicity of notation we will drop the subscript $ vi $ and simply denote the pose as $\\mathbf{T}_{k}$. We can then write our state to be estimated as the full set of pose matrices …","date":1639094400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639094400,"objectID":"60c66028016fa06e19bf194f61ad5d06","permalink":"https://mbrymer.github.io/project/camera_pose_estimation/","publishdate":"2021-12-10T00:00:00Z","relpermalink":"/project/camera_pose_estimation/","section":"project","summary":"Final assignment for AER 1513 - State Estimation for Robotics.","tags":["Sensor Fusion","State Estimation"],"title":"Batch Pose Estimation for a Stereo Camera","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://mbrymer.github.io/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]