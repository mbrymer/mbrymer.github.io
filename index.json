[{"authors":null,"categories":null,"content":"I’m an engineer who’s passionate about quadcopters, flight vehicles and just about anything that moves. Just graduated from UTIAS, looking for cool stuff to work on\nIpsum lorem sum stuff\nIpsum lorem sum\nThis is a picture\nDownload my resumé.\n","date":1607817600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1607817600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I’m an engineer who’s passionate about quadcopters, flight vehicles and just about anything that moves. Just graduated from UTIAS, looking for cool stuff to work on\nIpsum lorem sum stuff","tags":null,"title":"Matt Brymer","type":"authors"},{"authors":null,"categories":null,"content":"I’m an autonomy engineer who’s passionate about quadcopters, flight vehicles and just about anything that moves. This space is a showcase of some of the cool projects I’ve worked on recently. More about me here.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f8f9a3384bdb64736380a1b633f3701b","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I’m an autonomy engineer who’s passionate about quadcopters, flight vehicles and just about anything that moves. This space is a showcase of some of the cool projects I’ve worked on recently.","tags":null,"title":"","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://mbrymer.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":null,"categories":null,"content":"GitHub Repo | Project Report\nThis was my final project at UTIAS where I developed a visual-inertial estimation algorithm for estimating the relative pose of a quadrotor over a landing pad and deployed it on a custom hardware platform.\nOverview This work was performed as part of an overarching personal project to build a quadrotor from hobby grade components capable of autonomously taking off, flying to a target GPS location and then executing a precision landing on a landing pad at that location. One of the most crucial aspects of this system is deriving an accurate estimate of the vehicle relative pose for control of the final landing phase. Low cost GPS units typically only achieve horizontal position accuracies on the order of 2.5 m [1], which is good for general purpose or return to launch flight but insufficient for a precise landing.\nMore accurate measurements of the relative pose can be achieved via vision with an onboard camera, which has been well explored in the literature [2][3]. These pose measurements can then be fused with IMU data to achieve a high rate state estimate. Thus for this project I investigated developing a visual-inertial estimation algorithm for implementation on a low cost, hobby grade quadrotor.\nApproach Filter Architecture and Software Implementation To simplify the vision problem I placed an AprilTag fiducial marker on the landing pad, which are commonly used in robotics and allow the full 6 DoF pose to be estimated with a monocular camera [4]. The picture below shows the overall estimation problem and coordinate frames involved.\nI used a Multiplicative Extended Kalman Filter [5] to estimate both the relative pose and the IMU biases. This framework breaks the state up into a nominal and perturbation component that allows the attitude quaternion to be estimated with a conventional Kalman Filter, which is normally limited to vector spaces. The full details of the derivation are available in the report.\nI implemented the filter in a C++ ROS node, which takes IMU data and the pose of the tag from the AprilTag ROS node [6] as inputs and outputs the estimated relative pose and covariance. The detections reach the filter with a significant delay due to the cost of the image processing pipeline, which was on the order of 0.2 s in the hardware setup. For this reason the tag detections when received are fused to a past state based on the estimated delay.\nHardware Platform The filter was deployed on a custom quadrotor platform illustrated below. Flight control is performed by the Holybro Kakute F7 flight controller running ArduPilot 4.2.2 and the filter runs on a NVIDIA Jetson Nano 4GB using images from an industrial machine vision camera. The vehicle has a total mass of 1.115 kg and estimated maximum thrust of 36.9 N based on data from the motor manufacturer, leading to a thrust to weight ratio of 3.4.\nTo make the image processing time tractable, the camera image is downsampled to 640x480 to allow the AprilTag node to produce detections at 15 Hz. A custom landing target was designed using a bundle of 13 AprilTags of different sizes within a 0.84 m wide square, which allows for some tags to remain visible when the vehicle is offset laterally.\nValidation Simulation The filter was first validated in simulation using the TRAILab fork of the RotorS simulator [7] before testing in hardware. The flight was simulated using prerecorded open loop transmitter inputs for manual sweeping flights over the AprilTag target at heights of 2 m and 4 m. The video below for the 2 m case shows the estimated and ground truth pose visualized with RViz as well as the simulated camera view with the AprilTag detection overlaid.\nThe filter generally tracks the ground truth position and orientation well, with estimation errors below approximately 0.050 m as long as detection is maintained. The cameras loses sight of the tag multiple times during the run at the edges of its sweeping trajectory, leading to drift in the state estimate. In this example we at one point observe a drift of 0.58 m in the position estimate after 5 s of lost detection around 50 s into the event. While not ideal, this is a relatively long interval to go without correction from measurements.\nAdditional details of the orientation estimate and filter consistency are available in the report.\nHardware Testing The filter was then validated via outdoor flight tests on the quadrotor platform under manual flight. Two cases were evaluated, including a sweeping flight back and forth over the target as well as a full landing. The height in the sweeping flight case was approximately 2.5 m while the landing case began from a height of approximately 5 m.\nThe videos below show ground and onboard camera views along with the RViz visualization of the state estimate and time traces for both cases.\n$$ $$\nNo ground truth is available so the estimate is instead compared against the AprilTag reported pose for reference. While detection is maintained the filter …","date":1661731200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661731200,"objectID":"321852a82a4d4e09ccee8faeb3784f36","permalink":"https://mbrymer.github.io/project/quadrotor_relative_pose_estimation/","publishdate":"2022-08-29T00:00:00Z","relpermalink":"/project/quadrotor_relative_pose_estimation/","section":"project","summary":"Final project where I began developing a quadrotor capable of autonomously flying to a target location and landing based on visual-inertial navigation.","tags":["Sensor Fusion","Quadrotor","Computer Vision"],"title":"Visual-Inertial Relative Pose Estimation for Quadrotor Landing","type":"project"},{"authors":null,"categories":null,"content":"GitHub Repo | Project Report | Project Partners: Vandan Rao, Furqan Ahmed, Sangita Sahu\nThis was a course project where my teammates and I studied the problem of trajectory planning for high speed quadrotor flight in unknown environments.\nOverview Motion planning in environments that are unknown apriori is challenging for traditional sampling based planners because many samples end up becoming wasted when new obstacles invalidate entire sections of the tree. Numerous alternate approaches exist for this problem class, one of which is hierarchical planners. These use a fast global planner to generate a collision free path that is then refined by a local planner to generate a dynamically feasible trajectory, usually by solving an optimization problem. For this project we investigated these methods by implementing a simplified version of the FASTER planner [1].\nApproach For global planning, FASTER uses Jump Point Search (JPS) to generate a shortest piecewise linear path to the goal. It then performs a convex decomposition of the free space to generate a set of polytopes in which the plan can lie. The local plan is parameterized with a set of Bezier curves and an optimization problem is solved to find spline control points that minimize jerk and remain within vehicle dynamic limits. The convex hull property of Bezier curves reduces collision checking to keeping the control points within at least one polytope, but requires adding binary variables to allocate each segment to a polytope, promoting the problem to a MIQP.\nFASTER generates two plans, an optimistic one in both the free known and unknown space and a safe one in only the free known space with a stopping condition. This allows for higher speeds without sacrificing safety. To reduce scope for this project we planned only in the free known space, which sacrifices speed to maintain safety. The picture below shows our version, which we jokingly referred to as SLOWER.\nWe implemented the planner in a series of Python ROS nodes, one for each major step including the mapper (Furqan), global planner (Sangita), convex decomposition (Vandan) and local planner (myself). We tested it in Gazebo simulations making use of the quadrotor simulator used by the original authors in their GitHub repository[2].\nMy Contributions For my part, I lead the definition of the ROS architecture and software interfaces as well as writing the local planner and master node.\nThe local planner at each replanning iteration defines the optimization problem, calls the MOSEK optimization solver and then stores the result so that the time interpolated trajectory can be output at the high frequency needed for motion. This requires managing the full set of constraints including initial/boundary conditions, continuity between segments, vehicle dynamic limits(speed, acceleration and jerk), and planar collision constraints for each polytope as well as mixed continuous and binary variables.\nThe video below shows the local planner operating in isolation in the ROS + Gazebo environment using fixed global plan and convex decomposition inputs. The RViz view in the lower left shows the full spline trajectory generated by the local planner.\nFinal Results Due to implementation challenges with the global planner, it was necessary to integrate our convex decomposition and local planner with the JPS global planner implemented in the original FASTER implementation [2]. We tested this combination in both a small forest and office environment, which are illustrated below.\nA sample video of the flights in the both environments is shown below. In testing we achieved maximum speeds of up to 4.2 m/s and replanning rates of up to 10 Hz. The full results summary is available in the report here.\n$$ $$ Credits for the RViz tools to visualize the polyhedra and ellipsoids go to Sikang Liu [4].\nChallenges We encountered a number of challenges over the course of the project. The first was run time for the convex decomposition phase due to the cost of processing the incoming point cloud. Performance improved over the course of the project, but it can be seen in the videos that the decomposition still lags behind the speed of the vehicle. This delay causes the local planner to stop periodically while waiting for it to update.\nThe second was the time allocation heuristic. FASTER determines the time allocation for the trajectory by multiplying the minimum straight line time to the goal by a scale factor to account for path curvature. The scale factor is increased or decreased between replanning iterations depending on if a feasible solution is found in a form of line search to attempt to find the fastest possible trajectory time.\nThe use of infeasibility as the only stop criterion was found to not be robust and lead to scenarios where the factor was continually made more aggressive until the velocity traces saturated over long portions of the planning horizon. At these points the vehicle is moving so quickly that the optimization …","date":1651104e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651104e3,"objectID":"6c4d4780c50e497189cac3f0c9fe0822","permalink":"https://mbrymer.github.io/project/quadrotor_trajectory_planning/","publishdate":"2022-04-28T00:00:00Z","relpermalink":"/project/quadrotor_trajectory_planning/","section":"project","summary":"Course project for AER 1516 - Motion Planning for Robotics.","tags":["Trajectory Planning","Quadrotor"],"title":"Hierarchical Trajectory Planning for Quadrotor Flight in Unknown Environments","type":"project"},{"authors":null,"categories":null,"content":"The final assignment in one of my classes at UTIAS, AER 1513 - State Estimation for Robotics, had us solve a batch state estimation problem for a sensor head with both a stereo camera and an IMU attached.\nOverview The task was to derive an estimate for the full 6 DoF pose trajectory of the sensor head (henceforth referred to as the “vehicle”) for a sweeping motion spanning approximately 40 s given a history of IMU and stereo camera measurements. The vehicle was moved back and forth over a canvas with a set of markers on it that were each known to have a given, fixed location. These markers formed landmarks that through the stereo camera images and model could be used to establish an absolute position reference. Measurements to multiple landmarks can then be used to triangulate a position and orientation.\nThe experimental setup along with sample camera images and the ground truth pose is shown below.\nSome simplifications were made to reduced the scope of the problem. The linear acceleration signals were transformed to linear velocities and the stereo camera images were preprocessed to identify the locations of the landmarks within both images, removing the problem of data association. Thus the inputs for the estimation problem consisted of a time history of linear velocity, angular velocity and pixel coordinates for the landmarks in both camera images. Also available were ground truth pose measurements from an external motion capture system for evaluating estimation error, parameters of the stereo camera and the pose of the camera relative to the IMU.\nThe camera was kept relatively close to the canvas and moved through a range of angles, leading to only some of the landmarks being visible at any given time. For about a third of the event there were fewer than 3 landmarks in view, or sometimes none at all. Thus fusing both the camera and IMU data is necessary to obtain a pose estimate for the full trajectory.\nImplementation and Results The batch estimation problem was solved by forming a nonlinear least squares cost function in terms of the pose matrix at each timestep, the IMU inputs and stereo camera measurements. The problem was solved using an iterative Gauss-Newton optimization implemented in a MATLAB script that converged in approximately 10 iterations. Some of the details on the problem are given in the next section.\nA 3D trace of the final vehicle position estimate is shown below, along with the result from dead reckoning by solely integrating the IMU signal and the ground truth. It can be seen that the estimate overall does a good job of following the ground truth, although there does appear to be a consistent bias in the Y position. The dead reckoned estimate diverges from the ground truth relatively quickly, demonstrating the importance of supplementing IMU measurements with absolute references\nDetailed plots of the position and rotational estimation error when compared against ground truth are shown below. Overall the estimation errors are relatively low compared to the scale of motion in the problem, demonstrating the strength of the batch estimation approach. The peak translational estimation errors are about 30 mm in both X and Y and 22 mm in Z, compared to the range of motion of 1.3 m in X, 0.65 m in Y and 1.1 m in Z. The rotational estimation errors are similarly low, reaching maximums of about 3.5 deg in X, 3.75 deg in Y and 4.5 deg in Z.\nIt can be seen that the Y estimation error exhibits a clear bias towards negative error as we saw earlier in the 3D trace, with a mean error of approximately -15 mm. This is likely due to a bias in the stereo camera measurements, which was observed in provided histograms of the stereo camera measurement errors for the vertical pixel measurements.\nIt can also be seen that the $3-\\sigma$ uncertainty envelope calculated based on the estimated covariance bounds the estimation error for the most part. It grows when the number of landmarks in view decreases as expected, particularly between 20-23 s.\nOverall, the batch state estimation strategy did a great job of being able to fuse the IMU and stereo camera measurements to produce a robust pose estimate that is mostly consistent with its covariance estimate. This assignment was a great opportunity to practice some challenging Jacobian derivations, optimization problem formulation, rotation formalisms and sensor fusion for states that form a group rather than a vector space, such as pose matrices.\nProblem Formulation In this section I’ll briefly explain the methodology used to give a flavour for the problem. I’ll gloss over the finer points in the interest of keeping it short. For those who are interested the full details are given in Sections 6 and 7 of Barfoot[1].\nThe problem was formulated as a batch state estimation problem where we aim to estimate a trajectory of pose matrices $ \\mathbf{T}_{vi,k} $ for all discrete time steps within the interval, where the pose matrix encodes the world (i for inertial frame) …","date":1639094400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639094400,"objectID":"60c66028016fa06e19bf194f61ad5d06","permalink":"https://mbrymer.github.io/project/camera_pose_estimation/","publishdate":"2021-12-10T00:00:00Z","relpermalink":"/project/camera_pose_estimation/","section":"project","summary":"Final assignment for AER 1513 - State Estimation for Robotics.","tags":["Sensor Fusion","State Estimation"],"title":"Batch Pose Estimation for a Stereo Camera","type":"project"},{"authors":["Matt Brymer","吳恩達"],"categories":["Demo","教程"],"content":"Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It’s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more Get Started 👉 Create a new site 📚 Personalize your site 💬 Chat with the Wowchemy community or Hugo community 🐦 Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy 💡 Request a feature or report a bug for Wowchemy ⬆️ Updating Wowchemy? View the Update Tutorial and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n❤️ Click here to become a sponsor and help support Wowchemy’s future ❤️ As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features 🦄✨\nEcosystem Hugo Academic CLI: Automatically import publications from BibTeX Inspiration Check out the latest demo of what you’ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://mbrymer.github.io/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome 👋 We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","开源"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://mbrymer.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Matt Brymer","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"ff6a19061a984819d30c916886db56ef","permalink":"https://mbrymer.github.io/publication/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/example/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://mbrymer.github.io/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]