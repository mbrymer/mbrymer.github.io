[{"authors":null,"categories":null,"content":"I’m an engineer who’s passionate about quadcopters, flight vehicles and just about anything that moves. Just graduated from UTIAS, looking for cool stuff to work on\nIpsum lorem sum stuff\nIpsum lorem sum\nThis is a picture\nDownload my resumé.\n","date":1607817600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1607817600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I’m an engineer who’s passionate about quadcopters, flight vehicles and just about anything that moves. Just graduated from UTIAS, looking for cool stuff to work on\nIpsum lorem sum stuff","tags":null,"title":"Matt Brymer","type":"authors"},{"authors":null,"categories":null,"content":"I’m an autonomy engineer who’s passionate about quadcopters, flight vehicles and just about anything that moves. This space is a showcase of some of the cool projects I’ve worked on recently. More about me here.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f8f9a3384bdb64736380a1b633f3701b","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I’m an autonomy engineer who’s passionate about quadcopters, flight vehicles and just about anything that moves. This space is a showcase of some of the cool projects I’ve worked on recently.","tags":null,"title":"","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://mbrymer.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":null,"categories":null,"content":"GitHub Repo | Project Report\nThis was my final project at UTIAS where I developed a visual-inertial estimation algorithm for estimating the relative pose of a quadrotor over a landing pad and deployed it on a custom hardware platform.\nOverview This work was performed as part of an overarching personal project to build a quadrotor from hobby grade components capable of autonomously taking off, flying to a target GPS location and then executing a precision landing on a landing pad at that location. One of the most crucial aspects of this system is deriving an accurate estimate of the vehicle relative pose for control of the final landing phase. Low cost GPS units typically only achieve horizontal position accuracies on the order of 2.5 m [1], which is good for general purpose or return to launch flight but insufficient for a precise landing.\nMore accurate measurements of the relative pose can be achieved via vision with an onboard camera, which has been well explored in the literature [2][3]. These pose measurements can then be fused with IMU data to achieve a high rate state estimate. Thus for this project I investigated developing a visual-inertial estimation algorithm for implementation on a low cost, hobby grade quadrotor.\nApproach Filter Architecture and Software Implementation To simplify the vision problem I placed an AprilTag 3 fiducial marker on the landing pad, which is commonly used in robotics and allows the full 6 DoF pose to be estimated with a monocular camera [4]. The picture below shows the overall estimation problem and coordinate frames involved including the tag or inertial frame, $\\overset{\\rightharpoonup}{\\mathcal{F}_t}$ , the vehicle frame, $\\overset{\\rightharpoonup}{\\mathcal{F}_v}$ , and the camera frame, $\\overset{\\rightharpoonup}{\\mathcal{F}_c}$ .\nI used a Multiplicative Extended Kalman Filter [5] to estimate both the relative pose and the IMU biases. This framework breaks the state up into a nominal and perturbation component that allows the attitude in the form of a quaternion to be estimated with a conventional Kalman Filter, which is normally limited to vector spaces. The equations below show how this split is performed for the states, where $\\delta\\boldsymbol{\\theta}$ represents the rotation perturbation vector. The full details of the derivation are available in the report.\n$$ \\begin{align*} \u0026amp; \\textbf{r}_t^{vt} = \\bar{\\textbf{r}}_t^{vt} + \\delta\\textbf{r} \u0026amp; \u0026amp; \\textbf{v}_t^{vt} = \\bar{\\textbf{v}}_t^{vt} + \\delta\\textbf{v} \u0026amp; \\\\ \u0026amp; \\textbf{q}_{tv} = \\bar{\\textbf{q}}_{tv} \\otimes \\delta\\textbf{q} \u0026amp; \u0026amp; \\textbf{C}_{tv} = \\bar{\\textbf{C}}_{tv}\\delta \\textbf{C} \u0026amp; \\\\ \u0026amp; \\textbf{a}_b = \\bar{\\textbf{a}}_b + \\delta \\textbf{a}_b \u0026amp; \u0026amp; \\boldsymbol{\\omega}_b = \\bar{\\boldsymbol{\\omega}}_b + \\delta \\boldsymbol{\\omega}_b \u0026amp; \\\\ \u0026amp; \\delta\\textbf{q} = \\exp{\\frac{\\delta\\boldsymbol{\\theta}}{2}} \u0026amp; \u0026amp; \\delta \\textbf{C} = \\exp{[\\delta\\boldsymbol{\\theta}]_\\times} \u0026amp; \\end{align*} $$\nI implemented the filter in a C++ ROS node, which takes IMU data and the pose of the tag from the AprilTag ROS node [6] as inputs and outputs the estimated relative pose and covariance. The detections reach the filter with a significant delay due to the cost of the image processing pipeline, which was on the order of 0.2 s in the hardware setup. For this reason the tag detections when received are fused to a past state based on the estimated delay.\nHardware Platform The filter was deployed on a custom quadrotor platform illustrated below. Flight control is performed by the Holybro Kakute F7 flight controller running ArduPilot 4.2.2 and the filter runs on a NVIDIA Jetson Nano 4GB using images from an industrial machine vision camera. The vehicle has a total mass of 1.115 kg and estimated maximum thrust of 36.9 N based on data from the motor manufacturer, leading to a thrust to weight ratio of 3.4.\nTo make the image processing time tractable, the camera image is downsampled to 640x480 to allow the AprilTag node to produce detections at 15 Hz. A custom landing target was designed using a bundle of 13 AprilTags of different sizes within a 0.84 m wide square, which allows for some tags to remain visible when the vehicle is offset laterally.\nValidation Simulation The filter was first validated in simulation using the TRAILab fork of the RotorS simulator [7] before testing in hardware. The flight was simulated using prerecorded open loop transmitter inputs for manual sweeping flights over the AprilTag target at heights of 2 m and 4 m. The video below for the 2 m case shows the estimated and ground truth pose visualized with RViz as well as the simulated camera view with the AprilTag detection overlaid.\nThe filter generally tracks the ground truth position and orientation well, with estimation errors below approximately 0.050 m as long as detection is maintained. The cameras loses sight of the tag multiple times during the run at the edges of its sweeping trajectory, leading to drift in the state estimate. In this example we at one point …","date":1661731200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661731200,"objectID":"321852a82a4d4e09ccee8faeb3784f36","permalink":"https://mbrymer.github.io/project/quadrotor_relative_pose_estimation/","publishdate":"2022-08-29T00:00:00Z","relpermalink":"/project/quadrotor_relative_pose_estimation/","section":"project","summary":"Final project where I began developing a quadrotor capable of autonomously flying to a target location and landing based on visual-inertial navigation.","tags":["Sensor Fusion","Quadrotor","Computer Vision"],"title":"Visual-Inertial Relative Pose Estimation for Quadrotor Landing","type":"project"},{"authors":null,"categories":null,"content":"GitHub Repo | Project Report | Project Partners: Vandan Rao, Furqan Ahmed, Sangita Sahu\nThis was a course project where my teammates and I studied the problem of trajectory planning for high speed quadrotor flight in unknown environments.\nOverview Motion planning in environments that are unknown apriori and only discovered during motion is challenging for traditional random sampling planners(RRT* etc.) because it requires constantly updating the tree during motion as new obstacles are observed. Many samples end up becoming wasted when new obstacles invalidate entire sections of the tree. This reduces the feasible safe flight speed on computationally constrained platforms, such as quadrotors.\nNumerous alternate approaches exist for this problem class, one of which is hierarchical planners. These use a fast, low order global planner to generate a simple collision free path that is then refined by a local planner to generate a dynamically feasible trajectory over a receding planning horizon, usually by solving an optimization problem. For this project we investigated these methods by implementing a simplified version of the FASTER planner [1].\nApproach The full details of the algorithm are available in the original paper, but I’ll summarize it briefly here. For global planning FASTER uses Jump Point Search (JPS) to generate a collision free shortest piecewise linear path to the goal location. It then performs a convex decomposition of the free space surrounding each path segment to generate a set of polytopes within which the path can lie. The local plan is parameterized with a set of piecewise continuous Bezier curves and an optimization problem is solved to find a set of spline control points that minimize the squared jerk while maintaining continuity between segments, respecting vehicle dynamic limits and remaining collision free. The convex hull property of Bezier curves is used to reduce collision checking to keeping the control points of each segment within at least one polytope, simplifying it to a set of linear constraints. This requires adding binary variables to allocate each segment to a polytope, promoting the problem to a MIQP.\nFASTER generates two plans, one termed the full trajectory where it plans in both the free known and unknown space and another only in the free known space, termed the safe trajectory. The full trajectory is an optimistic plan that allows for higher speeds, but safety is maintained by being able to divert to a known collision free trajectory if a feasible solution cannot be found for the full trajectory. To reduce scope for this project we eliminated the full trajectory and planned only in the free known space, which sacrifices speed to maintain safety. The picture below shows our version, which we jokingly referred to as SLOWER.\nThe time allocated for the trajectory is another important consideration as lower time will drive higher vehicle speed and acceleration. FASTER handles this by assigning a constant time for the entire trajectory, evenly split between curve segments. The total time uses a heuristic that takes the minimum time to achieve the desired position/velocity/acceleration over the planning horizon and multiplies by a scale factor to account for path curvature. The scale factor is increased or decreased between replanning iterations depending on if a feasible solution was found in a form of line search to attempt to find the fastest possible trajectory time.\nWe implemented the planner in a series of Python ROS nodes, one for each major step including the mapper (Furqan), global planner (Sangita), convex decomposition (Vandan) and local planner (myself). We tested it in Gazebo simulations making use of the quadrotor simulator used by the original authors in their GitHub repository[2].\nMy Contributions For my part, I lead the definition of the ROS architecture and software interfaces as well as writing the local planner and master node.\nThe local planner at each replanning iteration defined the optimization problem, called the MOSEK optimization solver and then stored the result so that the time interpolated trajectory could be output at the higher frequency needed for motion. Replanning was performed at 10 Hz and the desired position and velocity were output at 100 Hz to the simulator.\nI also studied an alternate problem formulation where instead of the time allocation being set and path segments being able to float between intervals, the time allocation to each interval was made a decision variable and a fixed number of segments was used for each interval. The rationale was that jointly optimizing over time and path shape could possibly lead to faster trajectory times under given vehicle dynamic limits, similar to finding the racing line on a racecar. Adding segment times as a decision variable however induced a nonlinearity in the constraints, requiring this formulation to be solved using SQP.\nBoth formulations are summarized in detail in the report. Before …","date":1651104e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651104e3,"objectID":"6c4d4780c50e497189cac3f0c9fe0822","permalink":"https://mbrymer.github.io/project/quadrotor_trajectory_planning/","publishdate":"2022-04-28T00:00:00Z","relpermalink":"/project/quadrotor_trajectory_planning/","section":"project","summary":"Course project for AER 1516 - Motion Planning for Robotics.","tags":["Trajectory Planning","Quadrotor"],"title":"Hierarchical Trajectory Planning for Quadrotor Flight in Unknown Environments","type":"project"},{"authors":null,"categories":null,"content":"The final assignment in one of my classes at UTIAS, AER 1513 - State Estimation for Robotics, had us solve a batch state estimation problem for a sensor head with both a stereo camera and an IMU attached.\nOverview The task was to derive an estimate for the full 6 DoF pose trajectory of the sensor head (henceforth referred to as the “vehicle”) for a sweeping motion spanning approximately 40 s given a history of IMU and stereo camera measurements. The vehicle was moved back and forth over a canvas with a set of markers on it that were each known to have a given, fixed location. These markers formed landmarks that through the stereo camera images and model could be used to establish an absolute position reference. Measurements to multiple landmarks can then be used to triangulate a position and orientation.\nThe experimental setup along with sample camera images and the ground truth pose is shown below.\nSome simplifications were made to reduced the scope of the problem. The linear acceleration signals were transformed to linear velocities and the stereo camera images were preprocessed to identify the locations of the landmarks within both images, removing the problem of data association. Thus the inputs for the estimation problem consisted of a time history of linear velocity, angular velocity and pixel coordinates for the landmarks in both camera images. Also available were ground truth pose measurements from an external motion capture system for evaluating estimation error, parameters of the stereo camera and the pose of the camera relative to the IMU.\nThe camera was kept relatively close to the canvas and moved through a range of angles, leading to only some of the landmarks being visible at any given time. For about a third of the event there were fewer than 3 landmarks in view, or sometimes none at all. Thus fusing both the camera and IMU data is necessary to obtain a pose estimate for the full trajectory.\nImplementation and Results The batch estimation problem was solved by forming a nonlinear least squares cost function in terms of the pose matrix at each timestep, the IMU inputs and stereo camera measurements. The problem was solved using an iterative Gauss-Newton optimization implemented in a MATLAB script that converged in approximately 10 iterations. Some of the details on the problem are given in the next section.\nA 3D trace of the final vehicle position estimate is shown below, along with the result from dead reckoning by solely integrating the IMU signal and the ground truth. It can be seen that the estimate overall does a good job of following the ground truth, although there does appear to be a consistent bias in the Y position. The dead reckoned estimate diverges from the ground truth relatively quickly, demonstrating the importance of supplementing IMU measurements with absolute references\nDetailed plots of the position and rotational estimation error when compared against ground truth are shown below. Overall the estimation errors are relatively low compared to the scale of motion in the problem, demonstrating the strength of the batch estimation approach. The peak translational estimation errors are about 30 mm in both X and Y and 22 mm in Z, compared to the range of motion of 1.3 m in X, 0.65 m in Y and 1.1 m in Z. The rotational estimation errors are similarly low, reaching maximums of about 3.5 deg in X, 3.75 deg in Y and 4.5 deg in Z.\nIt can be seen that the Y estimation error exhibits a clear bias towards negative error as we saw earlier in the 3D trace, with a mean error of approximately -15 mm. This is likely due to a bias in the stereo camera measurements, which was observed in provided histograms of the stereo camera measurement errors for the vertical pixel measurements.\nIt can also be seen that the $3-\\sigma$ uncertainty envelope calculated based on the estimated covariance bounds the estimation error for the most part. It grows when the number of landmarks in view decreases as expected, particularly between 20-23 s.\nOverall, the batch state estimation strategy did a great job of being able to fuse the IMU and stereo camera measurements to produce a robust pose estimate that is mostly consistent with its covariance estimate. This assignment was a great opportunity to practice some challenging Jacobian derivations, optimization problem formulation, rotation formalisms and sensor fusion for states that form a group rather than a vector space, such as pose matrices.\nProblem Formulation In this section I’ll briefly explain the methodology used to give a flavour for the problem. I’ll gloss over the finer points in the interest of keeping it short. For those who are interested the full details are given in Sections 6 and 7 of Barfoot[1].\nThe problem was formulated as a batch state estimation problem where we aim to estimate a trajectory of pose matrices $ \\mathbf{T}_{vi,k} $ for all discrete time steps within the interval, where the pose matrix encodes the world (i for inertial frame) …","date":1639094400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639094400,"objectID":"60c66028016fa06e19bf194f61ad5d06","permalink":"https://mbrymer.github.io/project/camera_pose_estimation/","publishdate":"2021-12-10T00:00:00Z","relpermalink":"/project/camera_pose_estimation/","section":"project","summary":"Final assignment for AER 1513 - State Estimation for Robotics.","tags":["Sensor Fusion","State Estimation"],"title":"Batch Pose Estimation for a Stereo Camera","type":"project"},{"authors":["Matt Brymer","吳恩達"],"categories":["Demo","教程"],"content":"Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It’s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more Get Started 👉 Create a new site 📚 Personalize your site 💬 Chat with the Wowchemy community or Hugo community 🐦 Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy 💡 Request a feature or report a bug for Wowchemy ⬆️ Updating Wowchemy? View the Update Tutorial and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n❤️ Click here to become a sponsor and help support Wowchemy’s future ❤️ As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features 🦄✨\nEcosystem Hugo Academic CLI: Automatically import publications from BibTeX Inspiration Check out the latest demo of what you’ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://mbrymer.github.io/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome 👋 We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","开源"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://mbrymer.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Matt Brymer","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"ff6a19061a984819d30c916886db56ef","permalink":"https://mbrymer.github.io/publication/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/example/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://mbrymer.github.io/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]