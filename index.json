[{"authors":null,"categories":null,"content":"I’m an engineer who’s passionate about quadcopters, flight vehicles and just about anything that moves. Just graduated from UTIAS, looking for cool stuff to work on\nIpsum lorem sum stuff\nIpsum lorem sum\nThis is a picture\nDownload my resumé.\n","date":1607817600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1607817600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I’m an engineer who’s passionate about quadcopters, flight vehicles and just about anything that moves. Just graduated from UTIAS, looking for cool stuff to work on\nIpsum lorem sum stuff","tags":null,"title":"Matt Brymer","type":"authors"},{"authors":null,"categories":null,"content":"I’m an autonomy engineer who’s passionate about quadcopters, flight vehicles and just about anything that moves. This space is a showcase of some of the cool projects I’ve worked on recently. More about me here.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f8f9a3384bdb64736380a1b633f3701b","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I’m an autonomy engineer who’s passionate about quadcopters, flight vehicles and just about anything that moves. This space is a showcase of some of the cool projects I’ve worked on recently.","tags":null,"title":"","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://mbrymer.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":null,"categories":null,"content":"GitHub Repo | Project Report\nThis was my final project at UTIAS where I developed a visual-inertial estimation algorithm for estimating the relative pose of a quadrotor over a landing pad and deployed it on a custom hardware platform.\nOverview This work was performed as part of an overarching personal project to build a quadrotor from hobby grade components capable of autonomously taking off, flying to a target GPS location and then executing a precision landing on a landing pad at that location. One of the most crucial aspects of this system is deriving an accurate estimate of the vehicle relative pose for control of the final landing phase. Low cost GPS units typically only achieve horizontal position accuracies on the order of 2.5 m [1], which is good for general purpose or return to launch flight but insufficient for a precise landing.\nMore accurate measurements of the relative pose can be achieved via vision with an onboard camera, which has been well explored in the literature [2][3]. These pose measurements can then be fused with IMU data to achieve a high rate state estimate. Thus for this project I investigated developing a visual-inertial estimation algorithm for implementation on a low cost, hobby grade quadrotor.\nApproach Filter Architecture and Software Implementation To simplify the vision problem I placed an AprilTag 3 fiducial marker on the landing pad, which is commonly used in robotics and allows the full 6 DoF pose to be estimated with a monocular camera [4]. For this problem I assumed the landing pad remained fixed, so the quantities to be estimated are the position and orientation of the vehicle. The picture below shows the overall estimation problem and coordinate frames involved including the tag or inertial frame, $\\overset{\\rightharpoonup}{\\mathcal{F}_t}$ , the vehicle frame, $\\overset{\\rightharpoonup}{\\mathcal{F}_v}$ , and the camera frame, $\\overset{\\rightharpoonup}{\\mathcal{F}_c}$ .\nI used a Multiplicative Extended Kalman Filter[5] to estimate both the relative pose in the form of a position vector and attitude quaternion as well as the IMU biases. One of the key points is that this framework breaks the state up into a nominal and perturbation component that allows the rotation(Lie Group) to be estimated with a conventional Kalman Filter, which is normally limited to vector spaces. The perturbation component is used to capture the probabilistic uncertainty in the pose, and is expressed in the tangent space of the group(Lie Algebra) which forms a vector space in which the Kalman Filter operates and the corrections occur.\nThe equations below show how this nominal-perturbation split is performed for the states, where $\\delta\\boldsymbol{\\theta}$ represents the rotation perturbation vector. The full details of the derivation are available in the report for those interested.\n$$ \\begin{align*} \u0026amp; \\textbf{r}_t^{vt} = \\bar{\\textbf{r}}_t^{vt} + \\delta\\textbf{r} \u0026amp; \u0026amp; \\textbf{v}_t^{vt} = \\bar{\\textbf{v}}_t^{vt} + \\delta\\textbf{v} \u0026amp; \\\\ \u0026amp; \\textbf{q}_{tv} = \\bar{\\textbf{q}}_{tv} \\otimes \\delta\\textbf{q} \u0026amp; \u0026amp; \\textbf{C}_{tv} = \\bar{\\textbf{C}}_{tv}\\delta \\textbf{C} \u0026amp; \\\\ \u0026amp; \\textbf{a}_b = \\bar{\\textbf{a}}_b + \\delta \\textbf{a}_b \u0026amp; \u0026amp; \\boldsymbol{\\omega}_b = \\bar{\\boldsymbol{\\omega}}_b + \\delta \\boldsymbol{\\omega}_b \u0026amp; \\\\ \u0026amp; \\delta\\textbf{q} = \\exp{\\frac{\\delta\\boldsymbol{\\theta}}{2}} \u0026amp; \u0026amp; \\delta \\textbf{C} = \\exp{[\\delta\\boldsymbol{\\theta}]_\\times} \u0026amp; \\end{align*} $$\nI implemented the filter in a C++ ROS node, which takes IMU data and the pose of the detected tag bundle from the AprilTag ROS node [5] as inputs and performs the filtering steps to output the estimated relative pose and covariance. The tag detections reach the filter with a significant delay due to all the steps in the image processing pipeline, which was on the order of 0.15-0.25 s in the hardware setup. For this reason the state history is stored at each filter update and the tag detections when received are fused to a past state based on the estimated delay. After this the prior history is discarded and the prediction steps are recalculated to determine the estimate at the current time.\nHardware Platform The filter was deployed on a custom quadrotor platform illustrated below. Flight control is performed by the Holybro Kakute F7 flight controller running ArduPilot 4.2.2 and the filter runs on a NVIDIA Jetson Nano 4GB using images from an industrial machine vision camera. The vehicle has a total mass of 1.115 kg and estimated maximum thrust of 36.9 N based on data from the motor manufacturer, leading to a thrust to weight ratio of approximately 3.4.\nTo make the image processing time tractable, the camera image is downsampled to 640x480 to allow the AprilTag ROS node to produce detections at approximately 15 Hz. A custom landing target was designed consisting of a bundle of 13 AprilTags of different sizes within an overall 0.84 m wide square, which allows for some of the tags to remain visible while the vehicle is offset laterally. …","date":1661731200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661731200,"objectID":"321852a82a4d4e09ccee8faeb3784f36","permalink":"https://mbrymer.github.io/project/quadrotor_relative_pose_estimation/","publishdate":"2022-08-29T00:00:00Z","relpermalink":"/project/quadrotor_relative_pose_estimation/","section":"project","summary":"Final project where I began developing a quadrotor capable of autonomously flying to a target location and landing based on visual-inertial navigation.","tags":["Sensor Fusion","Quadrotor","Computer Vision"],"title":"Visual-Inertial Relative Pose Estimation for Quadrotor Landing","type":"project"},{"authors":null,"categories":null,"content":"GitHub Repo | Project Report | Project Partners: Vandan Rao, Furqan Ahmed, Sangita Sahu\nThis was a course project where my teammates and I studied the problem of trajectory planning for high speed quadrotor flight in unknown environments.\nOverview Motion planning in environments that are unknown apriori and only discovered during motion is challenging for traditional random sampling planners(RRT* etc.) because it requires constantly updating the tree during motion as new obstacles are observed. Many samples end up becoming wasted when new obstacles invalidate entire sections of the tree. This reduces the feasible safe flight speed on computationally constrained platforms, such as quadrotors.\nNumerous alternate approaches exist for this problem class, one of which is hierarchical planners. These use a fast, low order global planner to generate a simple collision free path that is then refined by a local planner to generate a dynamically feasible trajectory over a receding planning horizon, usually by solving an optimization problem. For this project we investigated these methods by implementing a simplified version of the FASTER planner [1].\nApproach The full details of the algorithm are available in the original paper, but I’ll summarize it briefly here. For global planning FASTER uses Jump Point Search (JPS) to generate a collision free shortest piecewise linear path to the goal location. It then performs a convex decomposition of the free space surrounding each path segment to generate a set of polytopes within which the path can lie. The local plan is parameterized with a set of piecewise continuous Bezier curves and an optimization problem is solved to find a set of spline control points that minimize the squared jerk while maintaining continuity between segments, respecting vehicle dynamic limits and remaining collision free. The convex hull property of Bezier curves is used to reduce collision checking to keeping the control points of each segment within at least one polytope, simplifying it to a set of linear constraints. This requires adding binary variables to allocate each segment to a polytope, promoting the problem to a MIQP.\nFASTER generates two plans, one termed the full trajectory where it plans in both the free known and unknown space and another only in the free known space, termed the safe trajectory. The full trajectory is an optimistic plan that allows for higher speeds, but safety is maintained by being able to divert to a known collision free trajectory if a feasible solution cannot be found for the full trajectory. To reduce scope for this project we eliminated the full trajectory and planned only in the free known space, which sacrifices speed to maintain safety. The picture below shows our version, which we jokingly referred to as SLOWER.\nThe time allocated for the trajectory is another important consideration as lower time will drive higher vehicle speed and acceleration. FASTER handles this by assigning a constant time for the entire trajectory, evenly split between curve segments. The total time uses a heuristic that takes the minimum time to achieve the desired position/velocity/acceleration over the planning horizon and multiplies by a scale factor to account for path curvature. The scale factor is increased or decreased between replanning iterations depending on if a feasible solution was found in a form of line search to attempt to find the fastest possible trajectory time.\nWe implemented the planner in a series of Python ROS nodes, one for each major step including the mapper (Furqan), global planner (Sangita), convex decomposition (Vandan) and local planner (myself). We tested it in Gazebo simulations making use of the quadrotor simulator used by the original authors in their GitHub repository[2].\nMy Contributions For my part, I lead the definition of the ROS architecture and software interfaces as well as writing the local planner and master node.\nThe local planner at each replanning iteration defined the optimization problem, called the MOSEK optimization solver and then stored the result so that the time interpolated trajectory could be output at the higher frequency needed for motion. Replanning was performed at 10 Hz and the desired position and velocity were output at 100 Hz to the simulator.\nI also studied an alternate problem formulation where instead of the time allocation being set and path segments being able to float between intervals, the time allocation to each interval was made a decision variable and a fixed number of segments was used for each interval. The rationale was that jointly optimizing over time and path shape could possibly lead to faster trajectory times under given vehicle dynamic limits, similar to finding the racing line on a racecar. Adding segment times as a decision variable however induced a nonlinearity in the constraints, requiring this formulation to be solved using SQP.\nBoth formulations are summarized in detail in the report. Before …","date":1651104e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651104e3,"objectID":"6c4d4780c50e497189cac3f0c9fe0822","permalink":"https://mbrymer.github.io/project/quadrotor_trajectory_planning/","publishdate":"2022-04-28T00:00:00Z","relpermalink":"/project/quadrotor_trajectory_planning/","section":"project","summary":"Course project for AER 1516 - Motion Planning for Robotics.","tags":["Trajectory Planning","Quadrotor"],"title":"Hierarchical Trajectory Planning for Quadrotor Flight in Unknown Environments","type":"project"},{"authors":null,"categories":null,"content":"The final assignment in one of my classes at UTIAS, AER 1513 - State Estimation for Robotics, had us solve a batch state estimation problem for a sensor head with both a stereo camera and an IMU attached.\nOverview The task was to derive an estimate for the full 6 DoF pose trajectory of the sensor head (henceforth referred to as the “vehicle”) for a sweeping motion spanning approximately 40 s given a history of IMU and stereo camera measurements. The vehicle was moved back and forth over a canvas with a set of markers on it that were each known to have a given, fixed location. These markers formed landmarks that through the stereo camera images and model could be used to establish an absolute position reference. Measurements to multiple landmarks can then be used to triangulate a position and orientation.\nThe experimental setup along with sample camera images and the ground truth pose is shown below.\nSome simplifications were made to reduced the scope of the problem. The linear acceleration signals were transformed to linear velocities and the stereo camera images were preprocessed to identify the locations of the landmarks within both images, removing the problem of data association. Thus the inputs for the estimation problem consisted of a time history of linear velocity, angular velocity and pixel coordinates for the landmarks in both camera images. Also available were ground truth pose measurements from an external motion capture system for evaluating estimation error, parameters of the stereo camera and the pose of the camera relative to the IMU.\nThe camera was kept relatively close to the canvas and moved through a range of angles, leading to only some of the landmarks being visible at any given time. For about a third of the event there were fewer than 3 landmarks in view, or sometimes none at all. Given that at least 3 landmarks are needed to fix a position fusing both the camera and IMU data is necessary to obtain a pose estimate for the full trajectory.\nImplementation and Results The batch estimation problem was solved by forming a nonlinear least squares cost function in terms of the pose matrix at each timestep, the IMU inputs and stereo camera measurements. The problem was solved using an iterative Gauss-Newton optimization in implemented in a MATLAB script that converged in approximately 10 iterations. The next section digs into some of the details, but for those with less time on their hands we go straight to the results first 🙂\nA 3D trace of the final vehicle position estimate is shown below, along with the result from dead reckoning by solely integrating the IMU signal and the ground truth. It can be seen that the estimate overall does a good job of following the ground truth, although there does appear to be a consistent bias in the Y position. The dead reckoned estimate diverges from the ground truth relatively quickly, demonstrating the importance of supplementing IMU measurements with absolute references\nDetailed plots of the position and rotational estimation error when compared against ground truth are shown below. Overall the estimation errors are relatively low compared to the scale of motion in the problem, demonstrating the strength of the batch estimation approach. The peak translational estimation errors are about 30 mm in both X and Y and 22 mm in Z, compared to the range of motion of 1.3 m in X, 0.65 m in Y and 1.1 m in Z. The rotational estimation errors are similarly low, reaching maximums of about 3.5 deg in X, 3.75 deg in Y and 4.5 deg in Z.\nIt can be seen that the Y estimation error exhibits a clear bias towards negative error as we saw earlier in the 3D trace, with a mean error of approximately -15 mm. This is likely due to a bias in the stereo camera measurements, which was observed in provided histograms of the stereo camera measurement errors for the vertical pixel measurements.\nIt can also be seen that the $3-\\sigma$ uncertainty envelope calculated based on the estimated covariance bounds the estimation error for the most part. It grows when the number of landmarks in view decreases as expected, particularly between 20-23 s.\nOverall, the batch state estimation strategy did a great job of being able to fuse the IMU and stereo camera measurements to produce a robust pose estimate that is mostly consistent with its covariance estimate. This assignment was a great opportunity to practice some challenging Jacobian derivations, optimization problem formulation, rotation formalisms and sensor fusion for states that form a group rather than a vector space, such as pose matrices.\nProblem Formulation In this section I’ll briefly explain the methodology used to give a flavour for the problem. I’ll gloss over the finer points in the interest of keeping it short. For those who are interested the full details are given in Sections 6 and 7 of Barfoot[1].\nThe problem was formulated as a batch state estimation problem where we aim to estimate a trajectory of pose matrices $ …","date":1639094400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639094400,"objectID":"60c66028016fa06e19bf194f61ad5d06","permalink":"https://mbrymer.github.io/project/camera_pose_estimation/","publishdate":"2021-12-10T00:00:00Z","relpermalink":"/project/camera_pose_estimation/","section":"project","summary":"Final assignment for AER 1513 - State Estimation for Robotics.","tags":["Sensor Fusion","State Estimation"],"title":"Batch Pose Estimation for a Stereo Camera","type":"project"},{"authors":["Matt Brymer","吳恩達"],"categories":["Demo","教程"],"content":"Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It’s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more Get Started 👉 Create a new site 📚 Personalize your site 💬 Chat with the Wowchemy community or Hugo community 🐦 Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy 💡 Request a feature or report a bug for Wowchemy ⬆️ Updating Wowchemy? View the Update Tutorial and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n❤️ Click here to become a sponsor and help support Wowchemy’s future ❤️ As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features 🦄✨\nEcosystem Hugo Academic CLI: Automatically import publications from BibTeX Inspiration Check out the latest demo of what you’ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://mbrymer.github.io/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome 👋 We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","开源"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://mbrymer.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://mbrymer.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":["Matt Brymer","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"ff6a19061a984819d30c916886db56ef","permalink":"https://mbrymer.github.io/publication/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/example/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://mbrymer.github.io/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]