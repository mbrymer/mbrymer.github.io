<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Projects | Matt Brymer</title><link>https://mbrymer.github.io/project/</link><atom:link href="https://mbrymer.github.io/project/index.xml" rel="self" type="application/rss+xml"/><description>Projects</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 29 Aug 2022 00:00:00 +0000</lastBuildDate><image><url>https://mbrymer.github.io/media/icon_hu1de1b76137c20b64cb0e309cf8549bbd_20734_512x512_fill_lanczos_center_3.png</url><title>Projects</title><link>https://mbrymer.github.io/project/</link></image><item><title>Visual-Inertial Relative Pose Estimation for Quadrotor Landing</title><link>https://mbrymer.github.io/project/quadrotor_relative_pose_estimation/</link><pubDate>Mon, 29 Aug 2022 00:00:00 +0000</pubDate><guid>https://mbrymer.github.io/project/quadrotor_relative_pose_estimation/</guid><description>&lt;p>&lt;a href="https://github.com/mbrymer/quadrotor_landing" target="_blank" rel="noopener">&lt;strong>GitHub Repo&lt;/strong>&lt;/a> &lt;strong>| &lt;a href="https://mbrymer.github.io/uploads/AER1810_Project_Report.pdf" target="_blank">Project Report&lt;/a>&lt;/strong>&lt;/p>
&lt;p>This was my final project at UTIAS where I developed a visual-inertial estimation algorithm for estimating the relative pose of a quadrotor over a landing pad and deployed it on a custom hardware platform.&lt;/p>
&lt;h2 id="overview">&lt;strong>Overview&lt;/strong>&lt;/h2>
&lt;p>This work was performed as part of an overarching personal project to build a quadrotor from hobby grade components capable of autonomously taking off, flying to a target GPS location and then executing a precision landing on a landing pad at that location. One of the most crucial aspects of this system is deriving an accurate estimate of the vehicle relative pose for control of the final landing phase. Low cost GPS units typically only achieve horizontal position accuracies on the order of 2.5 m [1], which is good for general purpose or return to launch flight but insufficient for a precise landing.&lt;/p>
&lt;p>More accurate measurements of the relative pose can be achieved via vision with an onboard camera, which has been well explored in the literature [2][3]. These pose measurements can then be fused with IMU data to achieve a high rate state estimate. Thus for this project I investigated developing a visual-inertial estimation algorithm for implementation on a low cost, hobby grade quadrotor.&lt;/p>
&lt;h2 id="approach">&lt;strong>Approach&lt;/strong>&lt;/h2>
&lt;h4 id="filter-architecture-and-software-implementation">Filter Architecture and Software Implementation&lt;/h4>
&lt;p>To simplify the vision problem I placed an AprilTag 3 fiducial marker on the landing pad, which is commonly used in robotics and allows the full 6 DoF pose to be estimated with a monocular camera [4]. For this problem I assumed the landing pad remained fixed, so the quantities to be estimated are the position and orientation of the vehicle. The picture below shows the overall estimation problem and coordinate frames involved including the tag or inertial frame, $\overset{\rightharpoonup}{\mathcal{F}_t}$ , the vehicle frame, $\overset{\rightharpoonup}{\mathcal{F}_v}$ , and the camera frame, $\overset{\rightharpoonup}{\mathcal{F}_c}$ .&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="state" srcset="
/project/quadrotor_relative_pose_estimation/images/state_illustration_final_hu7e0ae66df1eb436440d166f2f3f6faf2_1951896_d1be883e0b955da9a9c81435f668e7ae.webp 400w,
/project/quadrotor_relative_pose_estimation/images/state_illustration_final_hu7e0ae66df1eb436440d166f2f3f6faf2_1951896_46ff4a13eaca01462b88432e58d73c54.webp 760w,
/project/quadrotor_relative_pose_estimation/images/state_illustration_final_hu7e0ae66df1eb436440d166f2f3f6faf2_1951896_1200x1200_fit_q100_h2_lanczos_3.webp 1200w"
src="https://mbrymer.github.io/project/quadrotor_relative_pose_estimation/images/state_illustration_final_hu7e0ae66df1eb436440d166f2f3f6faf2_1951896_d1be883e0b955da9a9c81435f668e7ae.webp"
width="728"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>I used a Multiplicative Extended Kalman Filter[5] to estimate both the relative pose in the form of a position vector and attitude quaternion as well as the IMU biases. One of the key points is that this framework breaks the state up into a nominal and perturbation component that allows the rotation(Lie Group) to be estimated with a conventional Kalman Filter, which is normally limited to vector spaces. The perturbation component is used to capture the probabilistic uncertainty in the pose, and is expressed in the tangent space of the group(Lie Algebra) which forms a vector space in which the Kalman Filter operates and the corrections occur.&lt;/p>
&lt;p>The equations below show how this nominal-perturbation split is performed for the states, where $\delta\boldsymbol{\theta}$ represents the rotation perturbation vector. The full details of the derivation are available in the &lt;a href="https://mbrymer.github.io/uploads/AER1810_Project_Report.pdf" target="_blank">report&lt;/a> for those interested.&lt;/p>
&lt;p>$$
\begin{align*}
&amp;amp; \textbf{r}_t^{vt} = \bar{\textbf{r}}_t^{vt} + \delta\textbf{r} &amp;amp; &amp;amp; \textbf{v}_t^{vt} = \bar{\textbf{v}}_t^{vt} + \delta\textbf{v} &amp;amp; \\
&amp;amp; \textbf{q}_{tv} = \bar{\textbf{q}}_{tv} \otimes \delta\textbf{q} &amp;amp; &amp;amp; \textbf{C}_{tv} = \bar{\textbf{C}}_{tv}\delta \textbf{C} &amp;amp; \\
&amp;amp; \textbf{a}_b = \bar{\textbf{a}}_b + \delta \textbf{a}_b &amp;amp; &amp;amp; \boldsymbol{\omega}_b = \bar{\boldsymbol{\omega}}_b + \delta \boldsymbol{\omega}_b &amp;amp; \\
&amp;amp; \delta\textbf{q} = \exp{\frac{\delta\boldsymbol{\theta}}{2}} &amp;amp; &amp;amp; \delta \textbf{C} = \exp{[\delta\boldsymbol{\theta}]_\times} &amp;amp;
\end{align*}
$$&lt;/p>
&lt;p>I implemented the filter in a C++ ROS node, which takes IMU data and the pose of the detected tag bundle from the AprilTag ROS node [5] as inputs and performs the filtering steps to output the estimated relative pose and covariance. The tag detections reach the filter with a significant delay due to all the steps in the image processing pipeline, which was on the order of 0.15-0.25 s in the hardware setup. For this reason the state history is stored at each filter update and the tag detections when received are fused to a past state based on the estimated delay. After this the prior history is discarded and the prediction steps are recalculated to determine the estimate at the current time.&lt;/p>
&lt;h4 id="hardware-platform">Hardware Platform&lt;/h4>
&lt;p>The filter was deployed on a custom quadrotor platform illustrated below. Flight control is performed by the Holybro Kakute F7 flight controller running ArduPilot 4.2.2 and the filter runs on a NVIDIA Jetson Nano 4GB using images from an industrial machine vision camera. The vehicle has a total mass of 1.115 kg and estimated maximum thrust of 36.9 N based on data from the motor manufacturer, leading to a thrust to weight ratio of approximately 3.4.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="vehicle" srcset="
/project/quadrotor_relative_pose_estimation/images/hardware_platform_annotated_hud8a73b22106595b1eb79b8a149d1b67c_2975861_983060cad5de364e91b5fb8262cbd082.webp 400w,
/project/quadrotor_relative_pose_estimation/images/hardware_platform_annotated_hud8a73b22106595b1eb79b8a149d1b67c_2975861_c9ec6a45fbe2c9133342f30020de446c.webp 760w,
/project/quadrotor_relative_pose_estimation/images/hardware_platform_annotated_hud8a73b22106595b1eb79b8a149d1b67c_2975861_1200x1200_fit_q100_h2_lanczos.webp 1200w"
src="https://mbrymer.github.io/project/quadrotor_relative_pose_estimation/images/hardware_platform_annotated_hud8a73b22106595b1eb79b8a149d1b67c_2975861_983060cad5de364e91b5fb8262cbd082.webp"
width="760"
height="231"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>To make the image processing time tractable, the camera image is downsampled to 640x480 to allow the AprilTag ROS node to produce detections at approximately 15 Hz. A custom landing target was designed consisting of a bundle of 13 AprilTags of different sizes within an overall 0.84 m wide square, which allows for some of the tags to remain visible while the vehicle is offset laterally.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="target" srcset="
/project/quadrotor_relative_pose_estimation/images/target_hu5fed5c2255b04833853b631dec19a914_102918_c26f54fc6d0cf6679f0f6c4c67878a3d.webp 400w,
/project/quadrotor_relative_pose_estimation/images/target_hu5fed5c2255b04833853b631dec19a914_102918_d198509b1ffe3e65b65ba7a49bd4e625.webp 760w,
/project/quadrotor_relative_pose_estimation/images/target_hu5fed5c2255b04833853b631dec19a914_102918_1200x1200_fit_q100_h2_lanczos.webp 1200w"
src="https://mbrymer.github.io/project/quadrotor_relative_pose_estimation/images/target_hu5fed5c2255b04833853b631dec19a914_102918_c26f54fc6d0cf6679f0f6c4c67878a3d.webp"
width="449"
height="500"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h2 id="validation">&lt;strong>Validation&lt;/strong>&lt;/h2>
&lt;h4 id="simulation">Simulation&lt;/h4>
&lt;p>The filter was first validated in simulation using the TRAILab fork of the RotorS simulator [6] before testing in hardware. The simulator provides both the dynamics of the vehicle as well as implementing a low-level rate and attitude controller. The flight was simulated using prerecorded open loop transmitter inputs for manual sweeping flights over the AprilTag target at heights of 2 m and 4 m. The video below for the 2 m case shows the estimated and ground truth pose visualized with RViz as well as the simulated camera view with the AprilTag detection overlaid.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/_05GB9q91SE" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>The filter generally tracks the ground truth position and orientation well, with estimation errors below approximately 0.050 m as long as detection is maintained. The cameras loses sight of the tag multiple times during the run at the edges of its sweeping trajectory, leading to drift in the state estimate. In this example we at one point observe a drift of 0.58 m in the position estimate after 5 s of lost detection around 50 s into the event. While not ideal, this is a relatively long interval to go without correction from measurements.&lt;/p>
&lt;p>Additional details of the orientation estimate and filter consistency are available in the &lt;a href="https://mbrymer.github.io/uploads/AER1810_Project_Report.pdf" target="_blank">report&lt;/a>.&lt;/p>
&lt;h4 id="hardware-testing">Hardware Testing&lt;/h4>
&lt;p>The filter was then validated via outdoor flight tests on the quadrotor platform under manual flight. Two cases were evaluated, including a sweeping flight back and forth over the target as well as a full landing. The height in the sweeping flight case varied between 1 - 2.5 m while the landing case began from a height of approximately 4 m. &lt;strong>Update these numbers to match the videos&lt;/strong>.&lt;/p>
&lt;p>Videos of both cases are shown below. These show ground and onboard camera views on the left along with the RViz visualization of the state estimate and time traces on the right.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/w7Ft2ymGmfc" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>$$ $$&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/w7Ft2ymGmfc" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>No ground truth available. Predicts these values, I think they&amp;rsquo;re good.&lt;/p>
&lt;p>Lose detection at these points, leading to XYZ amount of drift&lt;/p>
&lt;p>The filter was additionally evaluated indoors when moving the vehicle around by hand, the results of which are summarized in the &lt;a href="https://mbrymer.github.io/uploads/AER1810_Project_Report.pdf" target="_blank">report&lt;/a>.&lt;/p>
&lt;h2 id="challenges">&lt;strong>Challenges&lt;/strong>&lt;/h2>
&lt;p>A number of challenges were encountered through development that currently limit performance of the filter:&lt;/p>
&lt;ul>
&lt;li>The delay in the pose measurements significantly affects filter performance as the pose drives what state the IMU prediction propagates from
&lt;ul>
&lt;li>Exact delay time is uncertain due to camera not supporting software trigger. Delay is currently estimated via approximate time stamp plus a constant offset&lt;/li>
&lt;li>A study on the effect of delay time is available in the &lt;a href="https://mbrymer.github.io/uploads/AER1810_Project_Report.pdf" target="_blank">report&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>The AprilTag detector has a tendency to produce spurious rotation estimates when a tag in the bundle is close to the edge of the image
&lt;ul>
&lt;li>Tag can be detected when not fully in image, but are clipped to lie in image skewing the orientation&lt;/li>
&lt;li>Filtering strategy employed was insufficient to entirely eliminate this issue&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Drift in the estimate on lost detections remains a challenge and is negatively influenced by both of the above issues
&lt;ul>
&lt;li>The lack of orientation feedback under lost detections also leads orientation errors to produce an acceleration bias due to gravity&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Visibility of the tag under bright sunlight, leading to lost detections at higher heights even when the tag is fully in view&lt;/li>
&lt;/ul>
&lt;h2 id="conclusions-and-future-work">&lt;strong>Conclusions and Future Work&lt;/strong>&lt;/h2>
&lt;p>Despite these challenges, I still managed to produce a filter capable of achieving good visual-inertial estimation performance as long as detections are maintained. The performance is sufficient to deploy with a relative position controller and state machine for executing relative motions and landing maneuvers under nominal conditions, which is the next phase of the project.&lt;/p>
&lt;p>Some of the planned future work on improving the performance of the relative pose filter includes:&lt;/p>
&lt;ul>
&lt;li>Add additional measurements to improve robustness to lost detections. Possible options include:
&lt;ul>
&lt;li>Orientation reference, possibly from a simpler IMU only method such as the Mahony filter [7]&lt;/li>
&lt;li>Barometer to provide height feedback, possibly with a bias term&lt;/li>
&lt;li>Possibly consider using a motor thrust model, which allows moving accelerometer measurements to measurement side of the EKF&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Updating to a more advanced filter architecture such as a Sigma Point or Iterated Extended Kalman Filter&lt;/li>
&lt;li>Improve rejection of bad AprilTag orientation detections&lt;/li>
&lt;li>Moving to GPU implementation of AprilTag detector to potentially reduce need for downsampling, increase run rate or reduce delay time&lt;/li>
&lt;/ul>
&lt;h2 id="references">&lt;strong>References&lt;/strong>&lt;/h2>
&lt;p>[1] u-blox &lt;a href="https://content.u-blox.com/sites/default/files/SAM-M8Q_DataSheet_%28UBX-16012619%29.pdf" target="_blank" rel="noopener">SAM-M8Q Data Sheet&lt;/a>&lt;/p>
&lt;p>[2] K. Ling, D. Chow, A. Das, and S. L. Waslander, â€œAutonomous maritime landings for low-cost
VTOL aerial vehicles,â€ in 2014 Canadian Conference on Computer and Robot Vision, pp. 32â€“39,
2014.&lt;/p>
&lt;p>[3] A. Paris, B. T. Lopez, and J. P. How, â€œDynamic landing of an autonomous quadrotor on a moving
platform in turbulent wind conditions.â€ &lt;a href="https://arxiv.org/abs/1909.11071" target="_blank" rel="noopener">https://arxiv.org/abs/1909.11071&lt;/a>, 2019.&lt;/p>
&lt;p>[4] M. Krogius, A. Haggenmiller, and E. Olson, â€œFlexible Layouts for Fiducial Tags,â€ in 2019
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1898â€“1903,
2019.&lt;/p>
&lt;p>[5] F. L. Markley, â€œAttitude Error Representations for Kalman Filtering,â€ Journal of Guidance,
Control, and Dynamics, vol. 26, no. 2, pp. 311â€“317, 2003.&lt;/p>
&lt;p>[6] F. Furrer, M. Burri, M. Achtelik, and R. Siegwart, Robot Operating System (ROS): The Complete
Reference (Volume 1), ch. RotorSâ€”A Modular Gazebo MAV Simulator Framework, pp. 595â€“625.
Cham: Springer International Publishing, 2016.&lt;/p>
&lt;p>[7] R. Mahony, T. Hamel, and J.-M. Pflimlin, â€œNonlinear complementary filters on the special orthogonal
group,â€ IEEE Transactions on Automatic Control, vol. 53, no. 5, pp. 1203â€“1218, 2008.&lt;/p></description></item><item><title>Hierarchical Trajectory Planning for Quadrotor Flight in Unknown Environments</title><link>https://mbrymer.github.io/project/quadrotor_trajectory_planning/</link><pubDate>Thu, 28 Apr 2022 00:00:00 +0000</pubDate><guid>https://mbrymer.github.io/project/quadrotor_trajectory_planning/</guid><description>&lt;p>&lt;a href="https://github.com/evandanrao/slower" target="_blank" rel="noopener">&lt;strong>GitHub Repo&lt;/strong>&lt;/a> &lt;strong>|&lt;/strong> &lt;strong>&lt;a href="https://mbrymer.github.io/uploads/AER_1516_Project_Report.pdf" target="_blank">Project Report&lt;/a>&lt;/strong> &lt;strong>| Project Partners&lt;/strong>: &lt;a href="https://www.linkedin.com/in/evandanrao/" target="_blank" rel="noopener">Vandan Rao&lt;/a>, Furqan Ahmed, Sangita Sahu&lt;/p>
&lt;p>This was a course project where my teammates and I studied the problem of trajectory planning for high speed quadrotor flight in unknown environments.&lt;/p>
&lt;h2 id="overview">&lt;strong>Overview&lt;/strong>&lt;/h2>
&lt;p>Motion planning in environments that are unknown apriori and only discovered during motion is challenging for traditional random sampling planners(RRT* etc.) because it requires constantly updating the tree during motion as new obstacles are observed. Many samples end up becoming wasted when new obstacles invalidate entire sections of the tree. This reduces the feasible safe flight speed on computationally constrained platforms, such as quadrotors.&lt;/p>
&lt;p>Numerous alternate approaches exist for this problem class, one of which is hierarchical planners. These use a fast, low order global planner to generate a simple collision free path that is then refined by a local planner to generate a dynamically feasible trajectory over a receding planning horizon, usually by solving an optimization problem. For this project we investigated these methods by implementing a simplified version of the FASTER planner [1].&lt;/p>
&lt;h2 id="approach">&lt;strong>Approach&lt;/strong>&lt;/h2>
&lt;p>The full details of the algorithm are available in the original paper, but I&amp;rsquo;ll summarize it briefly here. For global planning FASTER uses Jump Point Search (JPS) to generate a collision free shortest piecewise linear path to the goal location. It then performs a convex decomposition of the free space surrounding each path segment to generate a set of polytopes within which the path can lie. The local plan is parameterized with a set of piecewise continuous Bezier curves and an optimization problem is solved to find a set of spline control points that minimize the squared jerk while maintaining continuity between segments, respecting vehicle dynamic limits and remaining collision free. The convex hull property of Bezier curves is used to reduce collision checking to keeping the control points of each segment within at least one polytope, simplifying it to a set of linear constraints. This requires adding binary variables to allocate each segment to a polytope, promoting the problem to a MIQP.&lt;/p>
&lt;p>FASTER generates two plans, one termed the full trajectory where it plans in both the free known and unknown space and another only in the free known space, termed the safe trajectory. The full trajectory is an optimistic plan that allows for higher speeds, but safety is maintained by being able to divert to a known collision free trajectory if a feasible solution cannot be found for the full trajectory. To reduce scope for this project we eliminated the full trajectory and planned only in the free known space, which sacrifices speed to maintain safety. The picture below shows our version, which we jokingly referred to as SLOWER.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="slower" srcset="
/project/quadrotor_trajectory_planning/images/slower_hu25b34b4d479de711a3efee8f30a1e4b2_80976_53c37a23ed886e039d811206189d43ce.webp 400w,
/project/quadrotor_trajectory_planning/images/slower_hu25b34b4d479de711a3efee8f30a1e4b2_80976_a102f3574851b1d9455328c4bea3b277.webp 760w,
/project/quadrotor_trajectory_planning/images/slower_hu25b34b4d479de711a3efee8f30a1e4b2_80976_1200x1200_fit_q100_h2_lanczos_3.webp 1200w"
src="https://mbrymer.github.io/project/quadrotor_trajectory_planning/images/slower_hu25b34b4d479de711a3efee8f30a1e4b2_80976_53c37a23ed886e039d811206189d43ce.webp"
width="760"
height="516"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;!-- Collision checking occurs by making use of the convex hull property of Bezier curves, which states that all points on the curve lie within the convex hull of its control points. This means keeping all control points of one segment within at least one polytope guarantees that segment is collision free, reducing collision checking to a set of linear inequality constraints on the control point locations. This also means that each curve segment must be assigned to a particular polytope, which the authors do by adding binary variables indicating if a segment is assigned to a particular polytope. These binary variables are then used to turn on or off the corresponding collision constraints and constrained such that a segment must be assigned to at least one polytope. The authors refer to this approach as interval allocation. The added binary variables makes the optimization problem a Mixed Integer Quadratic Program(MIQP). -->
&lt;p>The time allocated for the trajectory is another important consideration as lower time will drive higher vehicle speed and acceleration. FASTER handles this by assigning a constant time for the entire trajectory, evenly split between curve segments. The total time uses a heuristic that takes the minimum time to achieve the desired position/velocity/acceleration over the planning horizon and multiplies by a scale factor to account for path curvature. The scale factor is increased or decreased between replanning iterations depending on if a feasible solution was found in a form of line search to attempt to find the fastest possible trajectory time.&lt;/p>
&lt;p>We implemented the planner in a series of Python ROS nodes, one for each major step including the mapper (Furqan), global planner (Sangita), convex decomposition (Vandan) and local planner (myself). We tested it in Gazebo simulations making use of the quadrotor simulator used by the original authors in their &lt;a href="https://github.com/mit-acl/faster" target="_blank" rel="noopener">GitHub repository&lt;/a>[2].&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="ros_architecture" srcset="
/project/quadrotor_trajectory_planning/images/ros_arch_huc57d92bee57b5fd95d785edff58d2f1a_74410_96bc047a42f5dde52436b99f909fa24f.webp 400w,
/project/quadrotor_trajectory_planning/images/ros_arch_huc57d92bee57b5fd95d785edff58d2f1a_74410_ec3b5112c31a00fb28aa1adb4b8a8ade.webp 760w,
/project/quadrotor_trajectory_planning/images/ros_arch_huc57d92bee57b5fd95d785edff58d2f1a_74410_1200x1200_fit_q100_h2_lanczos_3.webp 1200w"
src="https://mbrymer.github.io/project/quadrotor_trajectory_planning/images/ros_arch_huc57d92bee57b5fd95d785edff58d2f1a_74410_96bc047a42f5dde52436b99f909fa24f.webp"
width="760"
height="593"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h2 id="my-contributions">&lt;strong>My Contributions&lt;/strong>&lt;/h2>
&lt;p>For my part, I lead the definition of the ROS architecture and software interfaces as well as writing the local planner and master node.&lt;/p>
&lt;p>The local planner at each replanning iteration defined the optimization problem, called the MOSEK optimization solver and then stored the result so that the time interpolated trajectory could be output at the higher frequency needed for motion. Replanning was performed at 10 Hz and the desired position and velocity were output at 100 Hz to the simulator.&lt;/p>
&lt;p>I also studied an alternate problem formulation where instead of the time allocation being set and path segments being able to float between intervals, the time allocation to each interval was made a decision variable and a fixed number of segments was used for each interval. The rationale was that jointly optimizing over time and path shape could possibly lead to faster trajectory times under given vehicle dynamic limits, similar to finding the racing line on a racecar. Adding segment times as a decision variable however induced a nonlinearity in the constraints, requiring this formulation to be solved using SQP.&lt;/p>
&lt;p>Both formulations are summarized in detail in the report. Before full implementation I compared them on three simple scenarios including a simple 90 degree corner, a more aggressive one and an S shaped corridor as shown in the picture below. I found that while on the simple corner the SQP formulation performed similarly while also yielding the time allocation, it was less robust on the tight corner and corridor and required a different number of segments per interval for good performance. Because the environments are not known apriori, this number is hard to set. The MIQP on the other hand is more robust as the segments can be reallocated between intervals.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="test_cases" srcset="
/project/quadrotor_trajectory_planning/images/prob_form_test_cases_presn_hua6464c23f79ab2fca8d5ea6cc517e365_469058_17e06eb06d0d179885729240244d3662.webp 400w,
/project/quadrotor_trajectory_planning/images/prob_form_test_cases_presn_hua6464c23f79ab2fca8d5ea6cc517e365_469058_7c737a7ddec5790dc7e727b30a01261a.webp 760w,
/project/quadrotor_trajectory_planning/images/prob_form_test_cases_presn_hua6464c23f79ab2fca8d5ea6cc517e365_469058_1200x1200_fit_q100_h2_lanczos_3.webp 1200w"
src="https://mbrymer.github.io/project/quadrotor_trajectory_planning/images/prob_form_test_cases_presn_hua6464c23f79ab2fca8d5ea6cc517e365_469058_17e06eb06d0d179885729240244d3662.webp"
width="760"
height="399"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Based on these findings only the MIQP formulation was used in the full implementation. The video below shows the local planner operating in isolation in the ROS + Gazebo environment using fixed global plan and convex decomposition inputs. The RViz view in the lower left shows the full spline trajectory generated by the local planner.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/RyII8VYFfPo" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h2 id="final-results">&lt;strong>Final Results&lt;/strong>&lt;/h2>
&lt;p>Due to implementation challenges with the global planner, it was necessary to integrate our convex decomposition and local planner with the JPS global planner implemented in the original FASTER implementation [2]. We tested this combination in both a small forest and office environment, which are illustrated below.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="gazebo_sim_setup" srcset="
/project/quadrotor_trajectory_planning/images/gazebo_sim_setup_collage_hu67affc286978f2c1c142dc9864db8474_122611_f32f2b99cdc4bc01829c4aba771f5530.webp 400w,
/project/quadrotor_trajectory_planning/images/gazebo_sim_setup_collage_hu67affc286978f2c1c142dc9864db8474_122611_554f7acab1abae1149b8df43bde83bc8.webp 760w,
/project/quadrotor_trajectory_planning/images/gazebo_sim_setup_collage_hu67affc286978f2c1c142dc9864db8474_122611_1200x1200_fit_q100_h2_lanczos_3.webp 1200w"
src="https://mbrymer.github.io/project/quadrotor_trajectory_planning/images/gazebo_sim_setup_collage_hu67affc286978f2c1c142dc9864db8474_122611_f32f2b99cdc4bc01829c4aba771f5530.webp"
width="760"
height="332"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>A sample video of the flights in the both environments is shown below. In testing we achieved maximum speeds of up to 4.2 m/s and replanning rates of up to 10 Hz. The full results summary is available in the report &lt;a href="https://mbrymer.github.io/uploads/AER_1516_Project_Report.pdf" target="_blank">here&lt;/a>.&lt;/p>
&lt;p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/w_spn4Af6mE" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
$$ $$
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/iUugwpjmoVI" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;p>Credits for the RViz tools to visualize the polyhedra and ellipsoids go to Sikang Liu [4].&lt;/p>
&lt;h2 id="challenges">&lt;strong>Challenges&lt;/strong>&lt;/h2>
&lt;p>We encountered a number of challenges over the course of the project. The first was run time for the convex decomposition phase due to the cost of processing the incoming point cloud. Performance improved over the course of the project, but it can be seen in the videos that the decomposition still lags behind the speed of the vehicle. This delay causes the local planner to stop periodically while waiting for it to update.&lt;/p>
&lt;p>The second was the time allocation scale factor heuristic. It was found that the strategy of using infeasibility as the only stop criterion in the line search was not robust and could lead to scenarios where the factor is continually made more aggressive until the velocity traces saturate over long portions of the planning horizon. At these points the vehicle is moving so quickly that the optimization problem eventually becomes infeasible without violating the speed constraints and requires large cutbacks on the time allocation factor, creating an oscillatory mode.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="infeasibility" srcset="
/project/quadrotor_trajectory_planning/images/velocity_saturation_infeasibility_hub3f7012c9f20c712abfd93ba3efff918_20852_01c357f55a3e2907b7ab56fa623c8014.webp 400w,
/project/quadrotor_trajectory_planning/images/velocity_saturation_infeasibility_hub3f7012c9f20c712abfd93ba3efff918_20852_8a9f4365f28d0f3d746358ff3e1b679d.webp 760w,
/project/quadrotor_trajectory_planning/images/velocity_saturation_infeasibility_hub3f7012c9f20c712abfd93ba3efff918_20852_1200x1200_fit_q100_h2_lanczos_3.webp 1200w"
src="https://mbrymer.github.io/project/quadrotor_trajectory_planning/images/velocity_saturation_infeasibility_hub3f7012c9f20c712abfd93ba3efff918_20852_01c357f55a3e2907b7ab56fa623c8014.webp"
width="432"
height="288"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>The original authors did not mention this issue so it could be an issue of our implementation, however it seems inherent to the method. I attempted the use of soft speed constraints to alleviate the issue, but did not reach a robust solution in the time allotted.&lt;/p>
&lt;p>The last challenge was the effect of the local plan on the observability of the world. The optimization problem does not consider observability and thus does not attempt to keep the camera level or limit angular rates to reduce motion blur. It was found that this can lead to the planner commanding large vehicle angles briefly that could cause the convex decomposition to fail due to poor point cloud data.&lt;/p>
&lt;h2 id="conclusion">&lt;strong>Conclusion&lt;/strong>&lt;/h2>
&lt;p>In conclusion, we managed to produce implementations of the local planning and world decomposition components of a hierarchical quadrotor trajectory planner capable of generating flyable trajectories in simulation. While challenges and robustness issues remained, this was still a great opportunity for me to practice motion planning, solving problems via optimization and ROS implementation.&lt;/p>
&lt;h2 id="references">&lt;strong>References&lt;/strong>&lt;/h2>
&lt;p>[1] - J. Tordesillas and J. P. How, â€œFASTER: Fast and safe trajectory
planner for navigation in unknown environments,â€ IEEE Transactions
on Robotics, 2021.&lt;/p>
&lt;p>[2] - J. Tordesillas, â€œFASTER: Fast and Safe Trajectory Planner for Navigation in Unknown Environments.â€ &lt;a href="https://github.com/mit-acl/faster" target="_blank" rel="noopener">https://github.com/mit-acl/faster&lt;/a>, 2021.&lt;/p>
&lt;p>[3] - &lt;a href="https://gitlab.com/mit-acl/lab/acl-gazebo" target="_blank" rel="noopener">https://gitlab.com/mit-acl/lab/acl-gazebo&lt;/a>&lt;/p>
&lt;p>[4] - S. Liu, &amp;ldquo;MRSL Decomp Util ROS.&amp;rdquo; &lt;a href="https://github.com/sikang/DecompROS" target="_blank" rel="noopener">https://github.com/sikang/DecompROS&lt;/a>, 2018.&lt;/p></description></item><item><title>Batch Pose Estimation for a Stereo Camera</title><link>https://mbrymer.github.io/project/camera_pose_estimation/</link><pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate><guid>https://mbrymer.github.io/project/camera_pose_estimation/</guid><description>&lt;p>The final assignment in one of my classes at UTIAS, AER 1513 - State Estimation for Robotics, had us solve a batch state estimation problem for a sensor head with both a stereo camera and an IMU attached.&lt;/p>
&lt;h2 id="overview">&lt;strong>Overview&lt;/strong>&lt;/h2>
&lt;p>The task was to derive an estimate for the full 6 DoF pose trajectory of the sensor head (henceforth referred to as the &amp;ldquo;vehicle&amp;rdquo;) for a sweeping motion spanning approximately 40 s given a history of IMU and stereo camera measurements. The vehicle was moved back and forth over a canvas with a set of markers on it that were each known to have a given, fixed location. These markers formed landmarks that through the stereo camera images and model could be used to establish an absolute position reference. Measurements to multiple landmarks can then be used to triangulate a position and orientation.&lt;/p>
&lt;p>The experimental setup along with sample camera images and the ground truth pose is shown below.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="problem_overview" srcset="
/project/camera_pose_estimation/images/problem_overview_huede8265e7e9cf94f196130bbb2e61763_355699_318e8f9b9363c72baf3f1eea964c0017.webp 400w,
/project/camera_pose_estimation/images/problem_overview_huede8265e7e9cf94f196130bbb2e61763_355699_be866f8876998c669505d1d4e2528dc7.webp 760w,
/project/camera_pose_estimation/images/problem_overview_huede8265e7e9cf94f196130bbb2e61763_355699_1200x1200_fit_q100_h2_lanczos_3.webp 1200w"
src="https://mbrymer.github.io/project/camera_pose_estimation/images/problem_overview_huede8265e7e9cf94f196130bbb2e61763_355699_318e8f9b9363c72baf3f1eea964c0017.webp"
width="750"
height="563"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Some simplifications were made to reduced the scope of the problem. The linear acceleration signals were transformed to linear velocities and the stereo camera images were preprocessed to identify the locations of the landmarks within both images, removing the problem of data association. Thus the inputs for the estimation problem consisted of a time history of linear velocity, angular velocity and pixel coordinates for the landmarks in both camera images. Also available were ground truth pose measurements from an external motion capture system for evaluating estimation error, parameters of the stereo camera and the pose of the camera relative to the IMU.&lt;/p>
&lt;p>The camera was kept relatively close to the canvas and moved through a range of angles, leading to only some of the landmarks being visible at any given time. For about a third of the event there were fewer than 3 landmarks in view, or sometimes none at all. Given that at least 3 landmarks are needed to fix a position fusing both the camera and IMU data is necessary to obtain a pose estimate for the full trajectory.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="landmarks_visible" srcset="
/project/camera_pose_estimation/images/landmarks_vs_time_huba00a997a251167c910168911254becd_37720_d1cf525145a1251ac57d67adfef3779d.webp 400w,
/project/camera_pose_estimation/images/landmarks_vs_time_huba00a997a251167c910168911254becd_37720_aa02904f041775fbe33308c215e24074.webp 760w,
/project/camera_pose_estimation/images/landmarks_vs_time_huba00a997a251167c910168911254becd_37720_1200x1200_fit_q100_h2_lanczos_3.webp 1200w"
src="https://mbrymer.github.io/project/camera_pose_estimation/images/landmarks_vs_time_huba00a997a251167c910168911254becd_37720_d1cf525145a1251ac57d67adfef3779d.webp"
width="760"
height="482"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h2 id="implementation-and-results">&lt;strong>Implementation and Results&lt;/strong>&lt;/h2>
&lt;p>The batch estimation problem was solved by forming a nonlinear least squares cost function in terms of the pose matrix at each timestep, the IMU inputs and stereo camera measurements. The problem was solved using an iterative Gauss-Newton optimization in implemented in a MATLAB script that converged in approximately 10 iterations. The next section digs into some of the details, but for those with less time on their hands we go straight to the results first ðŸ™‚&lt;/p>
&lt;p>A 3D trace of the final vehicle position estimate is shown below, along with the result from dead reckoning by solely integrating the IMU signal and the ground truth. It can be seen that the estimate overall does a good job of following the ground truth, although there does appear to be a consistent bias in the Y position. The dead reckoned estimate diverges from the ground truth relatively quickly, demonstrating the importance of supplementing IMU measurements with absolute references&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="overall_results" srcset="
/project/camera_pose_estimation/images/batch_overall_hu61e0fcca124c9360c4638627d70ae5b4_71225_7aa304f7814721bf4817833481beeba4.webp 400w,
/project/camera_pose_estimation/images/batch_overall_hu61e0fcca124c9360c4638627d70ae5b4_71225_d221c7200d2101cf0d96ee763dde5559.webp 760w,
/project/camera_pose_estimation/images/batch_overall_hu61e0fcca124c9360c4638627d70ae5b4_71225_1200x1200_fit_q100_h2_lanczos_3.webp 1200w"
src="https://mbrymer.github.io/project/camera_pose_estimation/images/batch_overall_hu61e0fcca124c9360c4638627d70ae5b4_71225_7aa304f7814721bf4817833481beeba4.webp"
width="760"
height="482"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Detailed plots of the position and rotational estimation error when compared against ground truth are shown below. Overall the estimation errors are relatively low compared to the scale of motion in the problem, demonstrating the strength of the batch estimation approach. The peak translational estimation errors are about 30 mm in both X and Y and 22 mm in Z, compared to the range of motion of 1.3 m in X, 0.65 m in Y and 1.1 m in Z. The rotational estimation errors are similarly low, reaching maximums of about 3.5 deg in X, 3.75 deg in Y and 4.5 deg in Z.&lt;/p>
&lt;p>It can be seen that the Y estimation error exhibits a clear bias towards negative error as we saw earlier in the 3D trace, with a mean error of approximately -15 mm. This is likely due to a bias in the stereo camera measurements, which was observed in provided histograms of the stereo camera measurement errors for the vertical pixel measurements.&lt;/p>
&lt;p>It can also be seen that the $3-\sigma$ uncertainty envelope calculated based on the estimated covariance bounds the estimation error for the most part. It grows when the number of landmarks in view decreases as expected, particularly between 20-23 s.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="position_error" srcset="
/project/camera_pose_estimation/images/batch_estimate_position_error_hue088b4c1768016dec7747f60a0d69e8b_78858_ef519ec96037c996df6e87a1d5fc003c.webp 400w,
/project/camera_pose_estimation/images/batch_estimate_position_error_hue088b4c1768016dec7747f60a0d69e8b_78858_ed374182f081b6ef2ca71fa32f9d89be.webp 760w,
/project/camera_pose_estimation/images/batch_estimate_position_error_hue088b4c1768016dec7747f60a0d69e8b_78858_1200x1200_fit_q100_h2_lanczos_3.webp 1200w"
src="https://mbrymer.github.io/project/camera_pose_estimation/images/batch_estimate_position_error_hue088b4c1768016dec7747f60a0d69e8b_78858_ef519ec96037c996df6e87a1d5fc003c.webp"
width="760"
height="577"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="rotation_error" srcset="
/project/camera_pose_estimation/images/batch_estimate_rotational_error_hubdf1acf408a701e68f69507d2c85a7c7_86543_86ef8e247045b4718b0847154e8000fb.webp 400w,
/project/camera_pose_estimation/images/batch_estimate_rotational_error_hubdf1acf408a701e68f69507d2c85a7c7_86543_2ff4fb70805a6aa6779dd592017d29d8.webp 760w,
/project/camera_pose_estimation/images/batch_estimate_rotational_error_hubdf1acf408a701e68f69507d2c85a7c7_86543_1200x1200_fit_q100_h2_lanczos_3.webp 1200w"
src="https://mbrymer.github.io/project/camera_pose_estimation/images/batch_estimate_rotational_error_hubdf1acf408a701e68f69507d2c85a7c7_86543_86ef8e247045b4718b0847154e8000fb.webp"
width="760"
height="577"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Overall, the batch state estimation strategy did a great job of being able to fuse the IMU and stereo camera measurements to produce a robust pose estimate that is mostly consistent with its covariance estimate. This assignment was a great opportunity to practice some challenging Jacobian derivations, optimization problem formulation, rotation formalisms and sensor fusion for states that form a group rather than a vector space, such as pose matrices.&lt;/p>
&lt;h2 id="problem-formulation">&lt;strong>Problem Formulation&lt;/strong>&lt;/h2>
&lt;p>In this section I&amp;rsquo;ll briefly explain the methodology used to give a flavour for the problem. I&amp;rsquo;ll gloss over the finer points in the interest of keeping it short. For those who are interested the full details are given in Sections 6 and 7 of Barfoot[1].&lt;/p>
&lt;p>The problem was formulated as a batch state estimation problem where we aim to estimate a trajectory of pose matrices $ \mathbf{T}_{vi,k} $ for all discrete time steps within the interval, where the pose matrix encodes the world (i for inertial frame) to vehicle transform and consists of a rotation matrix and position vector as:&lt;/p>
&lt;p>$$
\mathbf{T}_{vi,k} = \begin{bmatrix}
\mathbf{C}_{vi,k} &amp;amp; -\mathbf{C}_{vi,k}\mathbf{r}_{i,k}^{vi} \\
\mathbf{0}^T &amp;amp; 1
\end{bmatrix}
$$&lt;/p>
&lt;p>Where $ \mathbf{r}_{i,k}^{vi} $ represents the position vector pointing from the world frame to the vehicle expressed in the inertial frame and the rotation matrix $\mathbf{C}_{vi,k}$ maps from the world to the vehicle frame at timestep $k$.&lt;/p>
&lt;p>For simplicity of notation we will drop the subscript $ vi $ and simply denote the pose as $\mathbf{T}_{k}$. We can then write our state to be estimated as the full set of pose matrices across the interval, ie $\mathbf{x} = { \mathbf{T}_{0},\mathbf{T}_{1},\dots,\mathbf{T}_{K}}$.&lt;/p>
&lt;p>For our state estimation problem we need to define both a motion and measurement model. We use a kinematic motion model based on integrating the IMU velocities as inputs and take the pixel coordinates in the stereo camera images as measurements. We can stack the linear and angular velocities into a single generalized velocity and pixel coordinates into a measurement vector as follows:
$$
\boldsymbol{\varpi}_{k} = \begin{bmatrix}
\mathbf{v}_{v,k}^{vi} \\
\boldsymbol{\omega}_{v,k}^{vi}
\end{bmatrix} ,
\mathbf{y}_{jk} = \begin{bmatrix}
u_{l,jk} \\
v_{l,jk} \\
u_{r,jk} \\
v_{r,jk} \\
\end{bmatrix}
$$&lt;/p>
&lt;p>Where the IMU velocities are expressed in the vehicle frame and the horizontal/vertical pixel coordinates are for the $j$th landmark, assuming it&amp;rsquo;s in view. We can now express our kinematic motion model relating the state at one timestep to the next and our measurement model relating the state at the $k$th timestep to its expected measurement as:&lt;/p>
&lt;p>$$
\begin{gather*}
\text{Motion Model}: \mathbf{T}_{k} = \exp(\Delta t \boldsymbol{\varpi}_{k-1}) \mathbf{T}_{k-1} = \Xi_k \mathbf{T}_{k-1} \\
\text{Measurement Model}: \mathbf{y}_{jk} = \frac{1}{z_{jk}} \mathbf{M}\mathbf{T}_{cv}\mathbf{T}_{k}\mathbf{p}_{j,i}^{p_ji}
\end{gather*}
$$&lt;/p>
&lt;p>The motion model consists of a first order Euler integration of the generalized velocity to determine a change in pose based on the exponetial mapping from the Lie Algebra to the Lie Group of the space of pose matrices, $SE(3)$. The measurement model is a standard stereo camera model relating the world position of the $j$th landmark, $ \mathbf{p}_{j,i}^{p_ji} $ to its expected pixel coordinates through the vehicle pose, $ \mathbf{T}_{k} $, the IMU to camera transform, $ \mathbf{T}_{cv} $ and the stereo camera intrinsics, $ \mathbf{M} $. For further details, see Sections 6 and 7 of Barfoot [1].&lt;/p>
&lt;p>We can formulate the batch state estimation as an optimization problem by defining error terms derived from both models and using them in a nonlinear least-squares cost function. For the motion model we can define the error at each timestep as the difference between the pose estimate at that timestep and the pose produced by propagating the pose from the previous timestep forward using the motion model. This difference can be mapped to the Lie Algebra using the inverse exponential map, rather than a pose matrix. For the measurement model the error is simply the difference between the measured landmark pixel coordinates and those predicted by the measurement model based on the pose estimates. The terms can be weighted by the inverse covariances associated with the errors in the motion and measurement models.&lt;/p>
&lt;p>The error terms and nonlinear least squares cost function can be expressed as
$$
\begin{gather*}
\text{Motion Model Error}: \mathbf{e}_{v,k} = \ln(\Xi_k \mathbf{T}_{k-1} \mathbf{T}_{k}^{-1})^\lor \\
\text{Measurement Model}: \mathbf{e}_{y,jk} = \mathbf{y}_{jk} - \frac{1}{z_{jk}} \mathbf{M}\mathbf{T}_{cv}\mathbf{T}_{k}\mathbf{p}_{j,i}^{p_ji} \\
\text{Cost Function}: J(x) = \sum_{k=0}^K\frac{1}{2}\mathbf{e}_{v,k}^T\mathbf{Q}_k^{-1}\mathbf{e}_{v,k}+\sum_{k=0}^K\sum_{j_k}\frac{1}{2}\mathbf{e}_{y,jk}^T\mathbf{R}_{jk}^{-1}\mathbf{e}_{y,jk}
\end{gather*}
$$&lt;/p>
&lt;p>Where $ \mathbf{Q}_k $ represents the covariance associated with the motion model at timestep $ k $, $ \mathbf{R}_{jk} $ represents the covariance associated with the $ j $th landmark at timestep $ k $ and the $ j $ summation occurs over all landmarks visible at that timestep.&lt;/p>
&lt;p>This optimization can be solved with an iterative Gauss-Newton approach by linearizing the error terms. Each pose matrix state can be expressed as the combination of an operating point and a perturbation vector expressed in the Lie Algebra space. The perturbation can be represented with a Taylor expansion of the exponential map and then linearized by taking only the first term of its Taylor expansion as:
$$
\mathbf{T}_{k} = \exp(\boldsymbol{\epsilon}_{k}^{\wedge})\mathbf{T}_{op,k} \approx (\mathbf{1}+\boldsymbol{\epsilon}_{k}^{\wedge})\mathbf{T}_{op,k}
$$&lt;/p>
&lt;p>This approximation can be substituted into both the motion and measurement models to transform the error terms into the sum of an error at the current operating point, $ \mathbf{e}(\mathbf{x_{op}}) $ and a linear term associated with $ \epsilon_{k} $. When substituted into our cost function above these perturbations at each timestep can be stacked and factored out to yield a single matrix cost function as:&lt;/p>
&lt;p>$$
\begin{gather*}
J(\boldsymbol{\epsilon}) = \frac{1}{2}(\mathbf{e}(\mathbf{x}_{op})-\mathbf{H}\boldsymbol{\epsilon})^T\mathbf{W}^{-1}(\mathbf{e}(\mathbf{x}_{op})-\mathbf{H}\boldsymbol{\epsilon}) \\
\boldsymbol{\epsilon} = \begin{bmatrix}
\boldsymbol{\epsilon}_{0} \\
\boldsymbol{\epsilon}_{1} \\
\vdots \\
\boldsymbol{\epsilon}_{K}
\end{bmatrix}
\end{gather*}
$$&lt;/p>
&lt;p>Where the details of $ \mathbf{H} $ and $ \mathbf{W} $ contain Jacobians associated with the motion and measurement models. These are the keys to the assignment and hence to not be shared with some possible future student ðŸ™‚&lt;/p>
&lt;p>This cost function is the same form as a standard weighted linear least squares problem, and can be solved via the Normal Equation to determine the full perturbation vector $ \boldsymbol{\epsilon} $ that minimizes cost. This perturbation vector can then be applied to the operating point at each timestep to yield an updated pose estimate as:&lt;/p>
&lt;p>$$
\mathbf{T}_{op,k,new} = \exp(\boldsymbol{\epsilon}_k^{*\wedge})\mathbf{T}_{op,k}
$$&lt;/p>
&lt;p>This forms the full Gauss-Newton step and can be iterated until convergence, yielding a final batch estimate for the pose at all timesteps.&lt;/p>
&lt;h2 id="references">&lt;strong>References&lt;/strong>&lt;/h2>
&lt;p>[1] - T. D. Barfoot, State Estimation for Robotics. Cambridge: Cambridge University Press, 2017.&lt;/p>
&lt;p>Image credits for the experimental setup go to Professor Tim Barfoot at the &lt;a href="http://asrl.utias.utoronto.ca/" target="_blank" rel="noopener">Autonomous Space Robotics Lab&lt;/a> at UTIAS.&lt;/p></description></item><item><title>Example Project</title><link>https://mbrymer.github.io/project/example/</link><pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate><guid>https://mbrymer.github.io/project/example/</guid><description>&lt;p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p>
&lt;p>Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p>
&lt;p>Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p>
&lt;p>Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p>
&lt;p>Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p></description></item></channel></rss>