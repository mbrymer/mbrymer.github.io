<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.6.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=author content="Matt Brymer"><meta name=description content="Final assignment for AER 1513 - State Estimation for Robotics."><link rel=alternate hreflang=en-us href=https://mbrymer.github.io/project/camera_pose_estimation/><meta name=theme-color content="#1565c0"><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.b9924e6cf2c04f2bf5a71f4771d883b0.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu1de1b76137c20b64cb0e309cf8549bbd_20734_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu1de1b76137c20b64cb0e309cf8549bbd_20734_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://mbrymer.github.io/project/camera_pose_estimation/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="og:site_name" content="Matt Brymer"><meta property="og:url" content="https://mbrymer.github.io/project/camera_pose_estimation/"><meta property="og:title" content="Batch Pose Estimation for a Stereo Camera | Matt Brymer"><meta property="og:description" content="Final assignment for AER 1513 - State Estimation for Robotics."><meta property="og:image" content="https://mbrymer.github.io/project/camera_pose_estimation/featured.png"><meta property="twitter:image" content="https://mbrymer.github.io/project/camera_pose_estimation/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2021-12-10T00:00:00+00:00"><meta property="article:modified_time" content="2021-12-10T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://mbrymer.github.io/project/camera_pose_estimation/"},"headline":"Batch Pose Estimation for a Stereo Camera","image":["https://mbrymer.github.io/project/camera_pose_estimation/featured.png"],"datePublished":"2021-12-10T00:00:00Z","dateModified":"2021-12-10T00:00:00Z","author":{"@type":"Person","name":"Matt Brymer"},"publisher":{"@type":"Organization","name":"Matt Brymer","logo":{"@type":"ImageObject","url":"https://mbrymer.github.io/media/icon_hu1de1b76137c20b64cb0e309cf8549bbd_20734_192x192_fill_lanczos_center_3.png"}},"description":"Final assignment for AER 1513 - State Estimation for Robotics."}</script><title>Batch Pose Estimation for a Stereo Camera | Matt Brymer</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=60c66028016fa06e19bf194f61ad5d06><script src=/js/wowchemy-init.min.613040fe4f2c0f007b4dcb64404201cb.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Matt Brymer</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Matt Brymer</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/about><span>About</span></a></li><li class=nav-item><a class=nav-link href=/#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/uploads/matt_brymer_resume.pdf><span>Resume</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><article class="article article-project"><div class="article-container pt-3"><h1>Batch Pose Estimation for a Stereo Camera</h1><div class=article-metadata><span class=article-date>Dec 10, 2021</span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary btn-page-header" href=/tag/sensor-fusion/>Sensor Fusion</a>
<a class="btn btn-outline-primary btn-page-header" href=/tag/state-estimation/>State Estimation</a></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:600px;max-height:398px><div style=position:relative><img src=/project/camera_pose_estimation/featured_hud129c8a025671f5464e2011f0abb6ee9_484239_720x2500_fit_q100_h2_lanczos_3.webp width=600 height=398 alt class=featured-image></div></div><div class=article-container><div class=article-style><p>The final assignment in one of my classes at UTIAS, AER 1513 - State Estimation for Robotics, had us solve a batch state estimation problem for a sensor head with both a stereo camera and an IMU attached.</p><h2 id=overview><strong>Overview</strong></h2><p>The task was to derive an estimate for the full 6 DoF pose trajectory of the sensor head (henceforth referred to as the &ldquo;vehicle&rdquo;) for a sweeping motion spanning approximately 40 s given a history of IMU and stereo camera measurements. The vehicle was moved back and forth over a canvas with a set of markers on it that were each known to have a given location. These formed landmarks that could be used to establish an absolute position reference and triangulate a position and orientation.</p><p>The experimental setup along with sample camera images and the ground truth pose measured by an external motion capture system is shown below.</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt=problem_overview srcset="/project/camera_pose_estimation/images/problem_overview_huede8265e7e9cf94f196130bbb2e61763_355699_318e8f9b9363c72baf3f1eea964c0017.webp 400w,
/project/camera_pose_estimation/images/problem_overview_huede8265e7e9cf94f196130bbb2e61763_355699_be866f8876998c669505d1d4e2528dc7.webp 760w,
/project/camera_pose_estimation/images/problem_overview_huede8265e7e9cf94f196130bbb2e61763_355699_1200x1200_fit_q100_h2_lanczos_3.webp 1200w" src=/project/camera_pose_estimation/images/problem_overview_huede8265e7e9cf94f196130bbb2e61763_355699_318e8f9b9363c72baf3f1eea964c0017.webp width=750 height=563 loading=lazy data-zoomable></div></div></figure></p><p>The camera was kept relatively close to the canvas and moved through a range of angles, leading to only some of the landmarks being visible at any given time. For about a third of the event there were fewer than 3 landmarks in view, or sometimes none at all. Thus fusing both the camera and IMU data is necessary to obtain a pose estimate for the full trajectory.</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt=landmarks_visible srcset="/project/camera_pose_estimation/images/landmarks_vs_time_huba00a997a251167c910168911254becd_37720_d1cf525145a1251ac57d67adfef3779d.webp 400w,
/project/camera_pose_estimation/images/landmarks_vs_time_huba00a997a251167c910168911254becd_37720_aa02904f041775fbe33308c215e24074.webp 760w,
/project/camera_pose_estimation/images/landmarks_vs_time_huba00a997a251167c910168911254becd_37720_1200x1200_fit_q100_h2_lanczos_3.webp 1200w" src=/project/camera_pose_estimation/images/landmarks_vs_time_huba00a997a251167c910168911254becd_37720_d1cf525145a1251ac57d67adfef3779d.webp width=760 height=482 loading=lazy data-zoomable></div></div></figure></p><h2 id=implementation-and-results><strong>Implementation and Results</strong></h2><p>The batch estimation problem was solved by forming a nonlinear least squares cost function in terms of the pose matrix at each timestep, the IMU inputs and stereo camera measurements. The problem was solved using an iterative Gauss-Newton optimization implemented in a MATLAB script that converged in approximately 10 iterations. Some of the details on the problem formulation are given in the next section.</p><p>A 3D trace of the final vehicle position estimate is shown below, along with the ground truth and a dead reckoned estimate from integrating the IMU signal. It can be seen that the estimate overall follows the ground truth well, although there is a consistent bias in the Y position. The dead reckoned estimate quickly diverges from the ground truth, demonstrating the importance of supplementing IMU data with absolute references.</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt=overall_results srcset="/project/camera_pose_estimation/images/batch_overall_hu61e0fcca124c9360c4638627d70ae5b4_71225_7aa304f7814721bf4817833481beeba4.webp 400w,
/project/camera_pose_estimation/images/batch_overall_hu61e0fcca124c9360c4638627d70ae5b4_71225_d221c7200d2101cf0d96ee763dde5559.webp 760w,
/project/camera_pose_estimation/images/batch_overall_hu61e0fcca124c9360c4638627d70ae5b4_71225_1200x1200_fit_q100_h2_lanczos_3.webp 1200w" src=/project/camera_pose_estimation/images/batch_overall_hu61e0fcca124c9360c4638627d70ae5b4_71225_7aa304f7814721bf4817833481beeba4.webp width=760 height=482 loading=lazy data-zoomable></div></div></figure></p><p>Detailed plots of the position and rotational estimation error when compared against ground truth are shown below. Overall the estimation errors are relatively low compared to the scale of motion in the problem, demonstrating the strength of the batch estimation approach.</p><p>It can be seen that the Y estimation error exhibits a clear bias towards negative error as we saw earlier in the 3D trace, with a mean error of approximately -15 mm. This is likely due to a bias in the stereo camera measurements, which was observed in provided histograms of the stereo camera measurement errors for the vertical pixel measurements.</p><p>It can also be seen that the $3-\sigma$ uncertainty envelope calculated based on the estimated covariance bounds the estimation error for the most part. As expected it grows when the number of landmarks in view decreases, particularly between 20-23 s.</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt=position_error srcset="/project/camera_pose_estimation/images/batch_estimate_position_error_hue088b4c1768016dec7747f60a0d69e8b_78858_ef519ec96037c996df6e87a1d5fc003c.webp 400w,
/project/camera_pose_estimation/images/batch_estimate_position_error_hue088b4c1768016dec7747f60a0d69e8b_78858_ed374182f081b6ef2ca71fa32f9d89be.webp 760w,
/project/camera_pose_estimation/images/batch_estimate_position_error_hue088b4c1768016dec7747f60a0d69e8b_78858_1200x1200_fit_q100_h2_lanczos_3.webp 1200w" src=/project/camera_pose_estimation/images/batch_estimate_position_error_hue088b4c1768016dec7747f60a0d69e8b_78858_ef519ec96037c996df6e87a1d5fc003c.webp width=760 height=577 loading=lazy data-zoomable></div></div></figure><figure><div class="d-flex justify-content-center"><div class=w-100><img alt=rotation_error srcset="/project/camera_pose_estimation/images/batch_estimate_rotational_error_hubdf1acf408a701e68f69507d2c85a7c7_86543_86ef8e247045b4718b0847154e8000fb.webp 400w,
/project/camera_pose_estimation/images/batch_estimate_rotational_error_hubdf1acf408a701e68f69507d2c85a7c7_86543_2ff4fb70805a6aa6779dd592017d29d8.webp 760w,
/project/camera_pose_estimation/images/batch_estimate_rotational_error_hubdf1acf408a701e68f69507d2c85a7c7_86543_1200x1200_fit_q100_h2_lanczos_3.webp 1200w" src=/project/camera_pose_estimation/images/batch_estimate_rotational_error_hubdf1acf408a701e68f69507d2c85a7c7_86543_86ef8e247045b4718b0847154e8000fb.webp width=760 height=577 loading=lazy data-zoomable></div></div></figure></p><p>Overall, the batch state estimation strategy did a great job of being able to fuse both data streams to produce a robust pose estimate that is mostly consistent with its covariance estimate. This assignment was a great opportunity to practice some challenging Jacobian derivations, optimization problem formulation, rotation formalisms and sensor fusion for states that form a group rather than a vector space, such as pose matrices.</p><h2 id=problem-formulation><strong>Problem Formulation</strong></h2><p>In this section I&rsquo;ll briefly explain the methodology used to give a flavour for the problem. I&rsquo;ll gloss over the finer points in the interest of keeping it short. For those who are interested the full details are given in Sections 6 and 7 of Barfoot [1].</p><p>Some simplifications were made to reduced the scope of the problem. The linear acceleration signals were transformed to linear velocities and the stereo camera images were preprocessed to identify the landmarks within both images, removing the problem of data association. Thus the inputs for the estimation problem consisted of a time history of linear velocity, angular velocity and pixel coordinates for the landmarks in both camera images.</p><p>The problem was formulated as a batch state estimation problem where we aim to estimate a trajectory of pose matrices $ \mathbf{T}_{vi,k} $ for all discrete time steps within the interval, where the pose matrix encodes the world (i for inertial frame) to vehicle transform and consists of a rotation matrix and position vector as:</p><p>$$
\mathbf{T}_{vi,k} = \begin{bmatrix}
\mathbf{C}_{vi,k} & -\mathbf{C}_{vi,k}\mathbf{r}_{i,k}^{vi} \\
\mathbf{0}^T & 1
\end{bmatrix}
$$</p><p>Where $ \mathbf{r}_{i,k}^{vi} $ represents the position vector pointing from the world frame to the vehicle expressed in the inertial frame and the rotation matrix $\mathbf{C}_{vi,k}$ maps from the world to the vehicle frame at timestep $k$.</p><p>For simplicity of notation we will drop the subscript $ vi $ and simply denote the pose as $\mathbf{T}_{k}$. We can then write our state to be estimated as the full set of pose matrices across the interval, ie $\mathbf{x} = { \mathbf{T}_{0},\mathbf{T}_{1},\dots,\mathbf{T}_{K}}$.</p><p>For our state estimation problem we need to define both a motion and measurement model. We use a kinematic motion model based on integrating the IMU velocities as inputs and take the pixel coordinates in the stereo camera images as measurements. We can stack the linear and angular velocities into a single generalized velocity and pixel coordinates into a measurement vector as follows:
$$
\boldsymbol{\varpi}_{k} = \begin{bmatrix}
\mathbf{v}_{v,k}^{vi} \\
\boldsymbol{\omega}_{v,k}^{vi}
\end{bmatrix} ,
\mathbf{y}_{jk} = \begin{bmatrix}
u_{l,jk} \\
v_{l,jk} \\
u_{r,jk} \\
v_{r,jk} \\
\end{bmatrix}
$$</p><p>Where the IMU velocities are expressed in the vehicle frame and the horizontal/vertical pixel coordinates are for the $j$th landmark, assuming it&rsquo;s in view. We can now express our kinematic motion model relating the state at one timestep to the next and our measurement model relating the state at the $k$th timestep to its expected measurement as:</p><p>$$
\begin{gather*}
\text{Motion Model}: \mathbf{T}_{k} = \exp(\Delta t \boldsymbol{\varpi}_{k-1}) \mathbf{T}_{k-1} = \Xi_k \mathbf{T}_{k-1} \\
\text{Measurement Model}: \mathbf{y}_{jk} = \frac{1}{z_{jk}} \mathbf{M}\mathbf{T}_{cv}\mathbf{T}_{k}\mathbf{p}_{j,i}^{p_ji}
\end{gather*}
$$</p><p>The motion model consists of a first order Euler integration of the generalized velocity to determine a change in pose based on the exponetial mapping from the Lie Algebra to the Lie Group of the space of pose matrices, $SE(3)$. The measurement model is a standard stereo camera model relating the world position of the $j$th landmark, $ \mathbf{p}_{j,i}^{p_ji} $ to its expected pixel coordinates through the vehicle pose, $ \mathbf{T}_{k} $, the IMU to camera transform, $ \mathbf{T}_{cv} $ and the stereo camera intrinsics, $ \mathbf{M} $. For further details, see Sections 6 and 7 of Barfoot [1].</p><p>We can formulate the batch state estimation as an optimization problem by defining error terms derived from both models and using them in a nonlinear least-squares cost function. For the motion model we can define the error at each timestep as the difference between the pose estimate at that timestep and the pose produced by propagating the pose from the previous timestep forward using the motion model. This difference can be mapped to the Lie Algebra using the inverse exponential map, rather than a pose matrix. For the measurement model the error is simply the difference between the measured landmark pixel coordinates and those predicted by the measurement model based on the pose estimates. The terms can be weighted by the inverse covariances associated with the errors in the motion and measurement models.</p><p>The error terms and nonlinear least squares cost function can be expressed as
$$
\begin{gather*}
\text{Motion Model Error}: \mathbf{e}_{v,k} = \ln(\Xi_k \mathbf{T}_{k-1} \mathbf{T}_{k}^{-1})^\lor \\
\text{Measurement Model}: \mathbf{e}_{y,jk} = \mathbf{y}_{jk} - \frac{1}{z_{jk}} \mathbf{M}\mathbf{T}_{cv}\mathbf{T}_{k}\mathbf{p}_{j,i}^{p_ji} \\
\text{Cost Function}: J(x) = \sum_{k=0}^K\frac{1}{2}\mathbf{e}_{v,k}^T\mathbf{Q}_k^{-1}\mathbf{e}_{v,k}+\sum_{k=0}^K\sum_{j_k}\frac{1}{2}\mathbf{e}_{y,jk}^T\mathbf{R}_{jk}^{-1}\mathbf{e}_{y,jk}
\end{gather*}
$$</p><p>Where $ \mathbf{Q}_k $ represents the covariance associated with the motion model at timestep $ k $, $ \mathbf{R}_{jk} $ represents the covariance associated with the $ j $th landmark at timestep $ k $ and the $ j $ summation occurs over all landmarks visible at that timestep.</p><p>This optimization can be solved with an iterative Gauss-Newton approach by linearizing the error terms. Each pose matrix state can be expressed as the combination of an operating point and a perturbation vector expressed in the Lie Algebra space. The perturbation can be represented with a Taylor expansion of the exponential map and then linearized by taking only the first term of its Taylor expansion as:
$$
\mathbf{T}_{k} = \exp(\boldsymbol{\epsilon}_{k}^{\wedge})\mathbf{T}_{op,k} \approx (\mathbf{1}+\boldsymbol{\epsilon}_{k}^{\wedge})\mathbf{T}_{op,k}
$$</p><p>This approximation can be substituted into both the motion and measurement models to transform the error terms into the sum of an error at the current operating point, $ \mathbf{e}(\mathbf{x_{op}}) $ and a linear term associated with $ \epsilon_{k} $. When substituted into our cost function above these perturbations at each timestep can be stacked and factored out to yield a single matrix cost function as:</p><p>$$
\begin{gather*}
J(\boldsymbol{\epsilon}) = \frac{1}{2}(\mathbf{e}(\mathbf{x}_{op})-\mathbf{H}\boldsymbol{\epsilon})^T\mathbf{W}^{-1}(\mathbf{e}(\mathbf{x}_{op})-\mathbf{H}\boldsymbol{\epsilon}) \\
\boldsymbol{\epsilon} = \begin{bmatrix}
\boldsymbol{\epsilon}_{0} \\
\boldsymbol{\epsilon}_{1} \\
\vdots \\
\boldsymbol{\epsilon}_{K}
\end{bmatrix}
\end{gather*}
$$</p><p>Where the details of $ \mathbf{H} $ and $ \mathbf{W} $ contain Jacobians associated with the motion and measurement models. These are the keys to the assignment and hence to not be shared with some possible future student 🙂</p><p>This cost function is the same form as a standard weighted linear least squares problem, and can be solved via the Normal Equation to determine the full perturbation vector $ \boldsymbol{\epsilon} $ that minimizes cost. This perturbation vector can then be applied to the operating point at each timestep to yield an updated pose estimate as:</p><p>$$
\mathbf{T}_{op,k,new} = \exp(\boldsymbol{\epsilon}_k^{*\wedge})\mathbf{T}_{op,k}
$$</p><p>This forms the full Gauss-Newton step and can be iterated until convergence, yielding a final batch estimate for the pose at all timesteps.</p><h2 id=references><strong>References</strong></h2><p>[1] - T. D. Barfoot, State Estimation for Robotics. Cambridge: Cambridge University Press, 2017.</p><p>Image credits for the experimental setup go to Professor Tim Barfoot at the <a href=http://asrl.utias.utoronto.ca/ target=_blank rel=noopener>Autonomous Space Robotics Lab</a> at UTIAS.</p></div><div class=article-tags><a class="badge badge-light" href=/tag/sensor-fusion/>Sensor Fusion</a>
<a class="badge badge-light" href=/tag/state-estimation/>State Estimation</a></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://mbrymer.github.io/project/camera_pose_estimation/&text=Batch%20Pose%20Estimation%20for%20a%20Stereo%20Camera" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://mbrymer.github.io/project/camera_pose_estimation/&t=Batch%20Pose%20Estimation%20for%20a%20Stereo%20Camera" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Batch%20Pose%20Estimation%20for%20a%20Stereo%20Camera&body=https://mbrymer.github.io/project/camera_pose_estimation/" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://mbrymer.github.io/project/camera_pose_estimation/&title=Batch%20Pose%20Estimation%20for%20a%20Stereo%20Camera" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Batch%20Pose%20Estimation%20for%20a%20Stereo%20Camera%20https://mbrymer.github.io/project/camera_pose_estimation/" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://mbrymer.github.io/project/camera_pose_estimation/&title=Batch%20Pose%20Estimation%20for%20a%20Stereo%20Camera" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://mbrymer.github.io/><img class="avatar mr-3 avatar-circle" src=/authors/matt/avatar_hua655ae9feff4e483bc3a43a3bfe1b2bb_6302285_270x270_fill_q100_lanczos_center.jpg alt="Matt Brymer"></a><div class=media-body><h5 class=card-title><a href=https://mbrymer.github.io/>Matt Brymer</a></h5><h6 class=card-subtitle>Autonomy Engineer</h6><ul class=network-icon aria-hidden=true><li><a href=mailto:brymer.matt@gmail.com><i class="fas fa-envelope"></i></a></li><li><a href=https://www.linkedin.com/in/mattbrymer/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=https://github.com/mbrymer target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul></div></div><div class="project-related-pages content-widget-hr"></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2022 Matt Brymer</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js type=module></script>
<script src=/en/js/wowchemy.min.54dd6e4d8f2e4b1d098381b57f18dd83.js></script></body></html>