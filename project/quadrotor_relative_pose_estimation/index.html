<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.6.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=author content="Matt Brymer"><meta name=description content="Final project where I began developing a quadrotor capable of autonomously flying to a target location and landing based on visual-inertial navigation."><link rel=alternate hreflang=en-us href=https://mbrymer.github.io/project/quadrotor_relative_pose_estimation/><meta name=theme-color content="#1565c0"><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.b9924e6cf2c04f2bf5a71f4771d883b0.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu1de1b76137c20b64cb0e309cf8549bbd_20734_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu1de1b76137c20b64cb0e309cf8549bbd_20734_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://mbrymer.github.io/project/quadrotor_relative_pose_estimation/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="og:site_name" content="Matt Brymer"><meta property="og:url" content="https://mbrymer.github.io/project/quadrotor_relative_pose_estimation/"><meta property="og:title" content="Visual-Inertial Relative Pose Estimation for Quadrotor Landing | Matt Brymer"><meta property="og:description" content="Final project where I began developing a quadrotor capable of autonomously flying to a target location and landing based on visual-inertial navigation."><meta property="og:image" content="https://mbrymer.github.io/project/quadrotor_relative_pose_estimation/featured.png"><meta property="twitter:image" content="https://mbrymer.github.io/project/quadrotor_relative_pose_estimation/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2022-08-29T00:00:00+00:00"><meta property="article:modified_time" content="2022-08-29T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://mbrymer.github.io/project/quadrotor_relative_pose_estimation/"},"headline":"Visual-Inertial Relative Pose Estimation for Quadrotor Landing","image":["https://mbrymer.github.io/project/quadrotor_relative_pose_estimation/featured.png"],"datePublished":"2022-08-29T00:00:00Z","dateModified":"2022-08-29T00:00:00Z","author":{"@type":"Person","name":"Matt Brymer"},"publisher":{"@type":"Organization","name":"Matt Brymer","logo":{"@type":"ImageObject","url":"https://mbrymer.github.io/media/icon_hu1de1b76137c20b64cb0e309cf8549bbd_20734_192x192_fill_lanczos_center_3.png"}},"description":"Final project where I began developing a quadrotor capable of autonomously flying to a target location and landing based on visual-inertial navigation."}</script><title>Visual-Inertial Relative Pose Estimation for Quadrotor Landing | Matt Brymer</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=321852a82a4d4e09ccee8faeb3784f36><script src=/js/wowchemy-init.min.613040fe4f2c0f007b4dcb64404201cb.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Matt Brymer</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Matt Brymer</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/about><span>About</span></a></li><li class=nav-item><a class=nav-link href=/#portfolio><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/uploads/matt_brymer_resume.pdf><span>Resume</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><article class="article article-project"><div class="article-container pt-3"><h1>Visual-Inertial Relative Pose Estimation for Quadrotor Landing</h1><div class=article-metadata><span class=article-date>Aug 29, 2022</span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary btn-page-header" href=/tag/sensor-fusion/>Sensor Fusion</a>
<a class="btn btn-outline-primary btn-page-header" href=/tag/quadrotor/>Quadrotor</a>
<a class="btn btn-outline-primary btn-page-header" href=/tag/computer-vision/>Computer Vision</a></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:270px><div style=position:relative><img src=/project/quadrotor_relative_pose_estimation/featured_hu537e97e71bfa5b086bc2181cd27192ad_3126483_720x2500_fit_q100_h2_lanczos_3.webp width=720 height=270 alt class=featured-image></div></div><div class=article-container><div class=article-style><p><a href=https://github.com/mbrymer/quadrotor_landing target=_blank rel=noopener><strong>GitHub Repo</strong></a> <strong>| <a href=/uploads/AER1810_Project_Report.pdf target=_blank>Project Report</a></strong></p><p>This was my final project at UTIAS where I developed a visual-inertial estimation algorithm for estimating the relative pose of a quadrotor over a landing pad and deployed it on a custom hardware platform.</p><h2 id=overview><strong>Overview</strong></h2><p>This work was performed as part of an overarching personal project to build a quadrotor from hobby grade components capable of autonomously taking off, flying to a target GPS location and then executing a precision landing on a landing pad at that location. One of the most crucial aspects of this system is deriving an accurate estimate of the vehicle relative pose for control of the final landing phase. Low cost GPS units typically only achieve horizontal position accuracies on the order of 2.5 m [1], which is good for general purpose or return to launch flight but insufficient for a precise landing.</p><p>More accurate measurements of the relative pose can be achieved via vision with an onboard camera, which has been well explored in the literature [2][3]. These pose measurements can then be fused with IMU data to achieve a high rate state estimate. Thus for this project I investigated developing a visual-inertial estimation algorithm for implementation on a low cost, hobby grade quadrotor.</p><h2 id=approach><strong>Approach</strong></h2><h4 id=filter-architecture-and-software-implementation>Filter Architecture and Software Implementation</h4><p>To simplify the vision problem I placed an AprilTag 3 fiducial marker on the landing pad, which is commonly used in robotics and allows the full 6 DoF pose to be estimated with a monocular camera [4]. For this problem I assumed the landing pad remained fixed, so the quantities to be estimated are the position and orientation of the vehicle. The picture below shows the overall estimation problem and coordinate frames involved including the tag or inertial frame, $\overset{\rightharpoonup}{\mathcal{F}_t}$ , the vehicle frame, $\overset{\rightharpoonup}{\mathcal{F}_v}$ , and the camera frame, $\overset{\rightharpoonup}{\mathcal{F}_c}$ .</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt=state srcset="/project/quadrotor_relative_pose_estimation/images/state_illustration_final_hu7e0ae66df1eb436440d166f2f3f6faf2_1951896_d1be883e0b955da9a9c81435f668e7ae.webp 400w,
/project/quadrotor_relative_pose_estimation/images/state_illustration_final_hu7e0ae66df1eb436440d166f2f3f6faf2_1951896_46ff4a13eaca01462b88432e58d73c54.webp 760w,
/project/quadrotor_relative_pose_estimation/images/state_illustration_final_hu7e0ae66df1eb436440d166f2f3f6faf2_1951896_1200x1200_fit_q100_h2_lanczos_3.webp 1200w" src=/project/quadrotor_relative_pose_estimation/images/state_illustration_final_hu7e0ae66df1eb436440d166f2f3f6faf2_1951896_d1be883e0b955da9a9c81435f668e7ae.webp width=728 height=760 loading=lazy data-zoomable></div></div></figure></p><p>I used a Multiplicative Extended Kalman Filter[5] to estimate both the relative pose in the form of a position vector and attitude quaternion as well as the IMU biases. One of the key points is that this framework breaks the state up into a nominal and perturbation component that allows the rotation(Lie Group) to be estimated with a conventional Kalman Filter, which is normally limited to vector spaces. The perturbation component is used to capture the probabilistic uncertainty in the pose, and is expressed in the tangent space of the group(Lie Algebra) which forms a vector space in which the Kalman Filter operates and the corrections occur.</p><p>The equations below show how this nominal-perturbation split is performed for the states, where $\delta\boldsymbol{\theta}$ represents the rotation perturbation vector. The full details of the derivation are available in the <a href=/uploads/AER1810_Project_Report.pdf target=_blank>report</a> for those interested.</p><p>$$
\begin{align*}
& \textbf{r}_t^{vt} = \bar{\textbf{r}}_t^{vt} + \delta\textbf{r} & & \textbf{v}_t^{vt} = \bar{\textbf{v}}_t^{vt} + \delta\textbf{v} & \\
& \textbf{q}_{tv} = \bar{\textbf{q}}_{tv} \otimes \delta\textbf{q} & & \textbf{C}_{tv} = \bar{\textbf{C}}_{tv}\delta \textbf{C} & \\
& \textbf{a}_b = \bar{\textbf{a}}_b + \delta \textbf{a}_b & & \boldsymbol{\omega}_b = \bar{\boldsymbol{\omega}}_b + \delta \boldsymbol{\omega}_b & \\
& \delta\textbf{q} = \exp{\frac{\delta\boldsymbol{\theta}}{2}} & & \delta \textbf{C} = \exp{[\delta\boldsymbol{\theta}]_\times} &
\end{align*}
$$</p><p>I implemented the filter in a C++ ROS node, which takes IMU data and the pose of the detected tag bundle from the AprilTag ROS node [5] as inputs and performs the filtering steps to output the estimated relative pose and covariance. The tag detections reach the filter with a significant delay due to all the steps in the image processing pipeline, which was on the order of 0.15-0.25 s in the hardware setup. For this reason the state history is stored at each filter update and the tag detections when received are fused to a past state based on the estimated delay. After this the prior history is discarded and the prediction steps are recalculated to determine the estimate at the current time.</p><h4 id=hardware-platform>Hardware Platform</h4><p>The filter was deployed on a custom quadrotor platform illustrated below. Flight control is performed by the Holybro Kakute F7 flight controller running ArduPilot 4.2.2 and the filter runs on a NVIDIA Jetson Nano 4GB using images from an industrial machine vision camera. The vehicle has a total mass of 1.115 kg and estimated maximum thrust of 36.9 N based on data from the motor manufacturer, leading to a thrust to weight ratio of approximately 3.4.</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt=vehicle srcset="/project/quadrotor_relative_pose_estimation/images/hardware_platform_annotated_hud8a73b22106595b1eb79b8a149d1b67c_2975861_983060cad5de364e91b5fb8262cbd082.webp 400w,
/project/quadrotor_relative_pose_estimation/images/hardware_platform_annotated_hud8a73b22106595b1eb79b8a149d1b67c_2975861_c9ec6a45fbe2c9133342f30020de446c.webp 760w,
/project/quadrotor_relative_pose_estimation/images/hardware_platform_annotated_hud8a73b22106595b1eb79b8a149d1b67c_2975861_1200x1200_fit_q100_h2_lanczos.webp 1200w" src=/project/quadrotor_relative_pose_estimation/images/hardware_platform_annotated_hud8a73b22106595b1eb79b8a149d1b67c_2975861_983060cad5de364e91b5fb8262cbd082.webp width=760 height=231 loading=lazy data-zoomable></div></div></figure></p><p>To make the image processing time tractable, the camera image is downsampled to 640x480 to allow the AprilTag ROS node to produce detections at approximately 15 Hz. A custom landing target was designed consisting of a bundle of 13 AprilTags of different sizes within an overall 0.84 m wide square, which allows for some of the tags to remain visible while the vehicle is offset laterally.</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt=target srcset="/project/quadrotor_relative_pose_estimation/images/target_hu5fed5c2255b04833853b631dec19a914_102918_c26f54fc6d0cf6679f0f6c4c67878a3d.webp 400w,
/project/quadrotor_relative_pose_estimation/images/target_hu5fed5c2255b04833853b631dec19a914_102918_d198509b1ffe3e65b65ba7a49bd4e625.webp 760w,
/project/quadrotor_relative_pose_estimation/images/target_hu5fed5c2255b04833853b631dec19a914_102918_1200x1200_fit_q100_h2_lanczos.webp 1200w" src=/project/quadrotor_relative_pose_estimation/images/target_hu5fed5c2255b04833853b631dec19a914_102918_c26f54fc6d0cf6679f0f6c4c67878a3d.webp width=449 height=500 loading=lazy data-zoomable></div></div></figure></p><h2 id=validation><strong>Validation</strong></h2><h4 id=simulation>Simulation</h4><p>The filter was first validated in simulation using the TRAILab fork of the RotorS simulator [6] before testing in hardware. The simulator provides both the dynamics of the vehicle as well as implementing a low-level rate and attitude controller. The flight was simulated using prerecorded open loop transmitter inputs for manual sweeping flights over the AprilTag target at heights of 2 m and 4 m. The video below for the 2 m case shows the estimated and ground truth pose visualized with RViz as well as the simulated camera view with the AprilTag detection overlaid.</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/w7Ft2ymGmfc style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><p>The filter generally tracks the ground truth position and orientation well, with estimation errors below approximately 0.050 m as long as detection is maintained. The cameras loses sight of the tag multiple times during the run at the edges of its sweeping trajectory, leading to drift in the state estimate. In this example we observe a drift of 0.61 m in the Y position estimate after 2.5 s of lost detection around X s into the event. While not ideal, this is a relatively long interval to go without correction from measurements.</p><p>Additional details of the orientation estimate and filter consistency are available in the <a href=/uploads/AER1810_Project_Report.pdf target=_blank>report</a>.</p><h4 id=hardware-testing>Hardware Testing</h4><p>The filter was then validated via outdoor flight tests on the quadrotor platform under manual flight. Two cases were evaluated, including a sweeping flight back and forth over the target as well as a full landing. The height in the sweeping flight case varied between 1 - 2.5 m while the landing case began from a height of approximately 4 m. <strong>Update these numbers to match the videos</strong>.</p><p>Videos of both cases are shown below. These show ground and onboard camera views on the left along with the RViz visualization of the state estimate and time traces on the right.</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/w7Ft2ymGmfc style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><p>$$ $$</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/w7Ft2ymGmfc style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><p>No ground truth available. Predicts these values, I think they&rsquo;re good.</p><p>Lose detection at these points, leading to XYZ amount of drift</p><p>The filter was additionally evaluated indoors when moving the vehicle around by hand, the results of which are summarized in the <a href=/uploads/AER1810_Project_Report.pdf target=_blank>report</a>.</p><h2 id=challenges><strong>Challenges</strong></h2><p>A number of challenges were encountered through development that currently limit performance of the filter:</p><ul><li>The delay in the pose measurements significantly affects filter performance as the pose drives what state the IMU prediction propagates from<ul><li>Exact delay time is uncertain due to camera not supporting software trigger. Delay is currently estimated via approximate time stamp plus a constant offset</li><li>A study on the effect of delay time is available in the <a href=/uploads/AER1810_Project_Report.pdf target=_blank>report</a>.</li></ul></li><li>The AprilTag detector has a tendency to produce spurious rotation estimates when a tag in the bundle is close to the edge of the image<ul><li>Tag can be detected when not fully in image, but are clipped to lie in image skewing the orientation</li><li>Filtering strategy employed was insufficient to entirely eliminate this issue</li></ul></li><li>Drift in the estimate on lost detections remains a challenge and is negatively influenced by both of the above issues<ul><li>The lack of orientation feedback under lost detections also leads orientation errors to produce an acceleration bias due to gravity</li></ul></li><li>Visibility of the tag under bright sunlight, leading to lost detections at higher heights even when the tag is fully in view</li></ul><h2 id=conclusions-and-future-work><strong>Conclusions and Future Work</strong></h2><p>Despite these challenges, I still managed to produce a filter capable of achieving good visual-inertial estimation performance as long as detections are maintained. The performance is sufficient to deploy with a relative position controller and state machine for executing relative motions and landing maneuvers under nominal conditions, which is the next phase of the project.</p><p>Some of the planned future work on improving the performance of the relative pose filter includes:</p><ul><li>Add additional measurements to improve robustness to lost detections. Possible options include:<ul><li>Orientation reference, possibly from a simpler IMU only method such as the Mahony filter [7]</li><li>Barometer to provide height feedback, possibly with a bias term</li><li>Possibly consider using a motor thrust model, which allows moving accelerometer measurements to measurement side of the EKF</li></ul></li><li>Updating to a more advanced filter architecture such as a Sigma Point or Iterated Extended Kalman Filter</li><li>Improve rejection of bad AprilTag orientation detections</li><li>Moving to GPU implementation of AprilTag detector to potentially reduce need for downsampling, increase run rate or reduce delay time</li></ul><h2 id=references><strong>References</strong></h2><p>[1] u-blox <a href=https://content.u-blox.com/sites/default/files/SAM-M8Q_DataSheet_%28UBX-16012619%29.pdf target=_blank rel=noopener>SAM-M8Q Data Sheet</a></p><p>[2] K. Ling, D. Chow, A. Das, and S. L. Waslander, “Autonomous maritime landings for low-cost
VTOL aerial vehicles,” in 2014 Canadian Conference on Computer and Robot Vision, pp. 32–39,
2014.</p><p>[3] A. Paris, B. T. Lopez, and J. P. How, “Dynamic landing of an autonomous quadrotor on a moving
platform in turbulent wind conditions.” <a href=https://arxiv.org/abs/1909.11071 target=_blank rel=noopener>https://arxiv.org/abs/1909.11071</a>, 2019.</p><p>[4] M. Krogius, A. Haggenmiller, and E. Olson, “Flexible Layouts for Fiducial Tags,” in 2019
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1898–1903,
2019.</p><p>[5] F. L. Markley, “Attitude Error Representations for Kalman Filtering,” Journal of Guidance,
Control, and Dynamics, vol. 26, no. 2, pp. 311–317, 2003.</p><p>[6] F. Furrer, M. Burri, M. Achtelik, and R. Siegwart, Robot Operating System (ROS): The Complete
Reference (Volume 1), ch. RotorS—A Modular Gazebo MAV Simulator Framework, pp. 595–625.
Cham: Springer International Publishing, 2016.</p><p>[7] R. Mahony, T. Hamel, and J.-M. Pflimlin, “Nonlinear complementary filters on the special orthogonal
group,” IEEE Transactions on Automatic Control, vol. 53, no. 5, pp. 1203–1218, 2008.</p></div><div class=article-tags><a class="badge badge-light" href=/tag/sensor-fusion/>Sensor Fusion</a>
<a class="badge badge-light" href=/tag/quadrotor/>Quadrotor</a>
<a class="badge badge-light" href=/tag/computer-vision/>Computer Vision</a></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://mbrymer.github.io/project/quadrotor_relative_pose_estimation/&text=Visual-Inertial%20Relative%20Pose%20Estimation%20for%20Quadrotor%20Landing" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://mbrymer.github.io/project/quadrotor_relative_pose_estimation/&t=Visual-Inertial%20Relative%20Pose%20Estimation%20for%20Quadrotor%20Landing" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Visual-Inertial%20Relative%20Pose%20Estimation%20for%20Quadrotor%20Landing&body=https://mbrymer.github.io/project/quadrotor_relative_pose_estimation/" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://mbrymer.github.io/project/quadrotor_relative_pose_estimation/&title=Visual-Inertial%20Relative%20Pose%20Estimation%20for%20Quadrotor%20Landing" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Visual-Inertial%20Relative%20Pose%20Estimation%20for%20Quadrotor%20Landing%20https://mbrymer.github.io/project/quadrotor_relative_pose_estimation/" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://mbrymer.github.io/project/quadrotor_relative_pose_estimation/&title=Visual-Inertial%20Relative%20Pose%20Estimation%20for%20Quadrotor%20Landing" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://mbrymer.github.io/><img class="avatar mr-3 avatar-circle" src=/authors/admin/avatar_hua655ae9feff4e483bc3a43a3bfe1b2bb_6302285_270x270_fill_q100_lanczos_center.jpg alt="Matt Brymer"></a><div class=media-body><h5 class=card-title><a href=https://mbrymer.github.io/>Matt Brymer</a></h5><h6 class=card-subtitle>Autonomy Engineer</h6><p class=card-text>I&rsquo;m passionate about quadcopters, flight vehicles and just about anything that moves</p><ul class=network-icon aria-hidden=true><li><a href=mailto:brymer.matt@gmail.com><i class="fas fa-envelope"></i></a></li><li><a href=https://www.linkedin.com/in/mattbrymer/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=https://github.com/mbrymer target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul></div></div><div class="project-related-pages content-widget-hr"></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2022 Matt Brymer. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js type=module></script>
<script src=/en/js/wowchemy.min.54dd6e4d8f2e4b1d098381b57f18dd83.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>